{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, argparse, shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "from itertools import groupby, count\n",
    "from collections import deque\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate for building complement of a DNA sequence\n",
    "compl = str.maketrans('ATGCNatgcn', 'TACGNatgcn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgHelpFormatter(argparse.HelpFormatter):\n",
    "    '''\n",
    "    Formatter adding default values to help texts.\n",
    "    '''\n",
    "    def __init__(self, prog):\n",
    "        super().__init__(prog)\n",
    "\n",
    "    ## https://stackoverflow.com/questions/3853722\n",
    "    #def _split_lines(self, text, width):\n",
    "    #   if text.startswith('R|'):\n",
    "    #       return text[2:].splitlines()  \n",
    "    #   # this is the RawTextHelpFormatter._split_lines\n",
    "    #   return argparse.HelpFormatter._split_lines(self, text, width)\n",
    "\n",
    "    def _get_help_string(self, action):\n",
    "        text = action.help\n",
    "        if  action.default is not None and \\\n",
    "            action.default != argparse.SUPPRESS and \\\n",
    "            'default:' not in text.lower():\n",
    "            text += ' (default: {})'.format(action.default)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args(args=None):\n",
    "    parser = argparse.ArgumentParser(description='Estimates read starts (transposase insertion sites) for ONT rapid libraries',\n",
    "                                     formatter_class=ArgHelpFormatter, \n",
    "                                     add_help=False)\n",
    "\n",
    "    main_group = parser.add_argument_group('Main Options')\n",
    "    main_group.add_argument('reads',\n",
    "                            nargs='+',\n",
    "                            help='fastq files or path to directories containing fastq files (recursion depth 1)')\n",
    "    main_group.add_argument('genome',\n",
    "                            help='Fasta file containing the genomic sequences that is searched for insertion sites.')\n",
    "    main_group.add_argument('adapter',\n",
    "                            help='Transposon Y adapter sequence')\n",
    "    main_group.add_argument('--prefix',\n",
    "                            help=\"filename for readstarts in tap seperated value (.tsv) format\",\n",
    "                            default=\"readstarts\")\n",
    "    #main_group.add_argument('--verbose_out_file',\n",
    "    #                        help=\"print detailed, human readable information about each read to this file\")\n",
    "    #main_group.add_argument('--verbose_col_width',\n",
    "    #                        help=\"column width for verbose output\",\n",
    "    #                        type=int,\n",
    "    #                        default=250)\n",
    "    main_group.add_argument('--plot',\n",
    "                            help='plot results of gaussian approximation',\n",
    "                            action='store_true')\n",
    "    main_group.add_argument('--circular',\n",
    "                            action=\"store_true\")\n",
    "    main_group.add_argument('--strip',\n",
    "                            help=\"number of bases stripped from alignments to cope with coincidently identical sequences\",\n",
    "                            type=int,\n",
    "                            default=5)\n",
    "    main_group.add_argument('--wordsize',\n",
    "                            help='',\n",
    "                            type=int,\n",
    "                            default=8)\n",
    "    main_group.add_argument('--max_dist',\n",
    "                            help='''max distance between an adapter and a genome alignment to perform pairwise-alignment \n",
    "                                 in order to identify the exact transition point''',\n",
    "                            type=int,\n",
    "                            default=100)\n",
    "    main_group.add_argument('--min_readlength',\n",
    "                            help=\"min length of reads to be analyzed\",\n",
    "                            type=int,\n",
    "                            default=500)\n",
    "    main_group.add_argument('--processes',\n",
    "                            type=int,\n",
    "                            default=6)\n",
    "    main_group.add_argument('--batchsize',\n",
    "                            type=int,\n",
    "                            default=8000)\n",
    "\n",
    "    filter_group = parser.add_argument_group('Filter Options')\n",
    "    filter_group.add_argument('--mean',\n",
    "                              help='mean of per-base difference of actual sequence length from read length',\n",
    "                              type=float)\n",
    "    filter_group.add_argument('--std',\n",
    "                              help='standard deviation of per-base difference of actual sequence length from read length',\n",
    "                              type=float)\n",
    "    filter_group.add_argument('--min_blen',\n",
    "                              help=\"min produced alignment length (including errors)\",\n",
    "                              type=int,\n",
    "                              default=20)\n",
    "    filter_group.add_argument('--min_adapter_blen',\n",
    "                              help=\"min produced alignment length (including errors)\",\n",
    "                              type=int,\n",
    "                              default=50)\n",
    "    filter_group.add_argument('--min_genome_blen',\n",
    "                              help=\"min produced alignment length (including errors)\",\n",
    "                              type=int,\n",
    "                              default=50)\n",
    "    filter_group.add_argument('--f_window',\n",
    "                              help=\"sequence window around the position of transition from \"+\\\n",
    "                                   \"the adapter sequence to the chromosome sequence\",\n",
    "                              type=int,\n",
    "                              default=7)\n",
    "    filter_group.add_argument('--f_max_w_err',\n",
    "                              help=\"max amount of errors (insertions, deletions, mismatches) \"+\\\n",
    "                                   \"within the specified sequence window\",\n",
    "                              type=int,\n",
    "                              default=3)\n",
    "    filter_group.add_argument('--f_max_mm_strech',\n",
    "                              help=\"max amount of deletions or insertions in the whole realignment\",\n",
    "                              type=int,\n",
    "                              default=3)\n",
    "\n",
    "    help_group = parser.add_argument_group('Help')\n",
    "    help_group.add_argument('-h', '--help', \n",
    "                            action='help', \n",
    "                            default=argparse.SUPPRESS,\n",
    "                            help='Show this help message and exit.')\n",
    "    if args:\n",
    "        return parser.parse_args(args)\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_minimap2(ref_fn, fq_fn, paf_fn):\n",
    "    cmd ='minimap2 -x map-ont -c --eqx --secondary=no -t 4 {} {} >{} 2> /dev/null'.format(ref_fn, fq_fn, paf_fn)\n",
    "    #print('Running:', cmd)\n",
    "    return os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_paf(fn, cigar=False):\n",
    "    usecols = list(range(12))\n",
    "    names = [\"qid\", \"qlen\", \"qst\", \"qen\", \"strand\", \"subj\", \n",
    "             \"slen\", \"sst\", \"sen\", \"mlen\", \"blen\", \"mapq\"]\n",
    "    dtype = {\"qid\": str, \"qlen\": np.int32, \"qst\": np.int32, \n",
    "             \"qen\": np.int32, \"strand\": str, \"subj\": str,\n",
    "             \"slen\": np.int32, \"sst\": np.int32, \"sen\": np.int32, \n",
    "             \"mlen\": np.int32, \"blen\": np.int32, \"mapq\": np.int32}\n",
    "    converters = {}\n",
    "    if cigar:\n",
    "        usecols.append(22)\n",
    "        names.append('cg')\n",
    "        converters['cg'] = lambda x: x.split(':')[-1]\n",
    "    return pd.read_csv(fn, sep='\\t', header=None,\n",
    "                       usecols=usecols,\n",
    "                       names=names,\n",
    "                       dtype=dtype,\n",
    "                       converters=converters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_seq(df, sel):\n",
    "    #probl_st_plus = sel_plus & (df[sel_plus].cg_ad.str.replace('D|I', 'X').str.rstrip('=').str.rsplit('X', n=1).str[-1].astype(int) < args.wordsize)\n",
    "    #probl_en_minus = sel_minus & (df[sel_minus].cg_ad.str.split('=', n=1).str[0].astype(int) < args.wordsize)\n",
    "    cg_ad = df.cg_ad.str.replace('D|I', 'X')\n",
    "    cg_gn = df.cg_gn.str.replace('D|I', 'X')\n",
    "    \n",
    "    sel_ = sel & (df.trans_order == 1.) & (cg_ad.str.rstrip('=').str.rsplit('X', n=1).str[-1].astype(np.float32) >= args.wordsize) # & (df.strand_ad == '+')\n",
    "    df.loc[sel_ , \n",
    "           'query_st'] = df[sel_].qen_ad\n",
    "    sel_ = sel & (df.trans_order == 1.) & (cg_gn.str.split('=', n=1).str[0].astype(np.float32) >= args.wordsize) # & (df.strand_ad == '+')\n",
    "    df.loc[sel_ , \n",
    "           'query_en'] = df[sel_].qst_gn\n",
    "    \n",
    "    sel_ = sel & (df.trans_order == -1.) & (cg_gn.str.rstrip('=').str.rsplit('X', n=1).str[-1].astype(np.float32) >= args.wordsize) # & (df.strand_ad == '+') \n",
    "    df.loc[sel_ , \n",
    "           'query_st'] = df[sel_].qen_gn\n",
    "    sel_ = sel & (df.trans_order == -1.) & (cg_ad.str.split('=', n=1).str[0].astype(np.float32) >= args.wordsize) # & (df.strand_ad == '+')\n",
    "    df.loc[sel_ , \n",
    "           'query_en'] = df[sel_].qst_ad\n",
    "    \n",
    "    #sel_ = sel & (df.strand_ad == '-') & (df.trans_order == 1.)\n",
    "    #df.loc[sel_ & (cg_ad.str.rstrip('=').str.rsplit('X', n=1).str[-1].astype(int) >= args.wordsize), \n",
    "    #       'query_st'] = df[sel_].qen_ad\n",
    "    #df.loc[sel_ & (cg_gn.str.split('=', n=1).str[0].astype(int) >= args.wordsize), \n",
    "    #       'query_en'] = df[sel_].qst_gn\n",
    "    \n",
    "    #sel_ = sel & (df.strand_ad == '-') & (df.trans_order == -1.)\n",
    "    #df.loc[sel_ & (cg_gn.str.rstrip('=').str.rsplit('X', n=1).str[-1].astype(int) >= args.wordsize), \n",
    "    #       'query_st'] = df[sel_].qen_gn\n",
    "    #df.loc[sel_ & (cg_ad.str.split('=', n=1).str[0].astype(int) >= args.wordsize), \n",
    "    #       'query_en'] = df[sel_].qst_ad\n",
    "    \n",
    "    df.loc[sel, 'query_st'] -= args.strip\n",
    "    df.loc[sel, 'query_en'] += args.strip\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_length_stats(df):\n",
    "    dfs = []\n",
    "    # add length of read seq that aligns as \"qalen\"\n",
    "    dfs.append((df[df.subj.notnull()].qen - df[df.subj.notnull()].qst).to_frame(name='qalen'))\n",
    "    # add length of genome seq that aligns as \"salen\"\n",
    "    dfs.append((df[df.subj.notnull()].sen - df[df.subj.notnull()].sst).abs().to_frame(name='salen'))\n",
    "    ## add match, mismatch, insertion and deletion counts\n",
    "    #two_groups = '(?P<digit>[0-9]*)(?P<letter>[=XID])'\n",
    "    #d = df[df.subj_gn.notnull()].cg_gn.str.extractall(two_groups)\n",
    "    #d.loc[:, 'digit'] = d.digit.astype(\"float32\")\n",
    "    #for letter in ['=', 'X', 'I', 'D']:\n",
    "    #    dfs.append(d.loc[d.letter == letter].groupby(level=0).agg('sum').rename(columns = {'digit':letter}))\n",
    "    stats = pd.concat(dfs, axis=1).fillna(0.)\n",
    "    data = (stats.salen - stats.qalen)/stats.qalen\n",
    "    args.mean, args.std = norm.fit(data)\n",
    "    if args.plot:\n",
    "        fig, ax = plt.subplots(figsize=(8,6))\n",
    "        ax.hist(data, bins=100, density=True)\n",
    "        xmin, xmax = ax.get_xlim()\n",
    "        x = np.linspace(xmin, xmax, 200)\n",
    "        y = norm.pdf(x, args.mean, args.std)\n",
    "        ax.plot(x, y)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_args(['barcode16.fastq', 'MIT52.fa', 'pUC19_ISCg1.fa', '--prefix', 'barcode16', '--wordsize', '6', '--plot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fq_files = []\n",
    "for entry in args.reads:\n",
    "    if os.path.isfile(entry) and (entry.endswith(\".fastq\") or entry.endswith(\".fq\")):\n",
    "        fq_files.append(entry)\n",
    "    else:\n",
    "        fq_files.extend([os.path.join(entry, f) for f in os.listdir(entry) if os.path.isfile(os.path.join(entry, f)) \\\n",
    "            and (f.endswith(\".fastq\") or f.endswith(\".fq\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - reading adapter sequence ...\n",
      "          1 adapter sequence(s) in fasta file\n",
      " - reading genome fasta ...\n",
      "          1 genomic sequence(s) in fasta file\n",
      " - reading fastq files ...\n",
      "     272410 reads in dataset\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adapter = {}\n",
    "print(\" - reading adapter sequence ...\")\n",
    "for record in SeqIO.parse(args.adapter, \"fasta\"):\n",
    "    adapter[str(record.id)] = str(record.seq)\n",
    "adapter = pd.DataFrame.from_dict(adapter, orient='index', columns=['seq'], dtype='string')\n",
    "print('{:>11} adapter sequence(s) in fasta file'.format(len(adapter)))\n",
    "\n",
    "print(\" - reading genome fasta ...\")\n",
    "genome = {}\n",
    "for record in SeqIO.parse(args.genome, \"fasta\"):\n",
    "    genome[str(record.id)] = str(record.seq)\n",
    "genome = pd.DataFrame.from_dict(genome, orient='index', columns=['seq'], dtype='string')\n",
    "print('{:>11} genomic sequence(s) in fasta file'.format(len(genome)))\n",
    "\n",
    "print(\" - reading fastq files ...\")\n",
    "reads = {}\n",
    "#reads = pd.DataFrame([], {'seq':Seq})\n",
    "for fqFile in fq_files:\n",
    "    for record in SeqIO.parse(fqFile, \"fastq\"):\n",
    "        reads[str(record.id)] = str(record.seq)\n",
    "reads = pd.DataFrame.from_dict(reads, orient='index', columns=['seq'], dtype='string')\n",
    "print(\"{:>11} reads in dataset\\n\".format(len(reads)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame([(rid, len(seq)) for rid,seq in reads.items()], columns=['rid', 'rlen']).set_index('rid', drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dir = '{}_tmp'.format(args.prefix)\n",
    "if os.path.exists(tmp_dir):\n",
    "    shutil.rmtree(tmp_dir)\n",
    "os.makedirs(tmp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - performing reads to adapter reference mapping ...\n",
      "     175673         primary alignments against adapter sequence(s)\n",
      "      58740  33.4 % against (+) strand\n",
      "     116933  66.6 % against (-) strand\n"
     ]
    }
   ],
   "source": [
    "print(\" - performing reads to adapter reference mapping ...\")\n",
    "fq_fn = \" \".join(fq_files)\n",
    "ref_fn = args.adapter\n",
    "paf_fn = os.path.join(tmp_dir, \"adapter_alignment.paf\")\n",
    "exit_code = run_minimap2(ref_fn, fq_fn, paf_fn)\n",
    "if exit_code:\n",
    "    print('ERROR: adapter reference mapping failed with exit code', exit_code)\n",
    "    exit(1)\n",
    "ad_algn_df = parse_paf(paf_fn, cigar=True)#.set_index('qid')\n",
    "print(\"{:>11} {:>7} primary alignments against adapter sequence(s)\".format(len(ad_algn_df), \"\"))\n",
    "c = sum(ad_algn_df.strand == '+')\n",
    "print(\"{:>11} {:>5.1f} % against (+) strand\".format(c, c/len(ad_algn_df)*100.))\n",
    "c = sum(ad_algn_df.strand == '-')\n",
    "print(\"{:>11} {:>5.1f} % against (-) strand\".format(c, c/len(ad_algn_df)*100.))\n",
    "#df = pd.merge(df, algn_df, how='outer', left_index=True, right_index=True, sort=False, suffixes=('_x', '_y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - performing reads to genome reference mapping ...\n",
      "     143593         primary alignments against genomic sequence(s)\n",
      "      95609  66.6 % against (+) strand\n",
      "      47984  33.4 % against (-) strand\n"
     ]
    }
   ],
   "source": [
    "print(\" - performing reads to genome reference mapping ...\")\n",
    "ref_fn = args.genome\n",
    "paf_fn = os.path.join(tmp_dir, \"genome_alignment.paf\")\n",
    "exit_code = run_minimap2(ref_fn, fq_fn, paf_fn)\n",
    "if exit_code:\n",
    "    print('ERROR: adapter reference mapping failed with exit code', exit_code)\n",
    "    exit(1)\n",
    "gn_algn_df = parse_paf(paf_fn, cigar=True)#.set_index('qid')\n",
    "print(\"{:>11} {:>7} primary alignments against genomic sequence(s)\".format(len(gn_algn_df), \"\"))\n",
    "c = sum(gn_algn_df.strand == '+')\n",
    "print(\"{:>11} {:>5.1f} % against (+) strand\".format(c, c/len(gn_algn_df)*100.))\n",
    "c = sum(gn_algn_df.strand == '-')\n",
    "print(\"{:>11} {:>5.1f} % against (-) strand\".format(c, c/len(gn_algn_df)*100.))\n",
    "#df = pd.merge(df, algn_df, how='outer', left_index=True, right_index=True, sort=False, suffixes=('_ad', '_gn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - determining statistics about per-base difference ([align. subject len] - [align. query len]) / [align. query len] al from genome alignments\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.0079 mean of per-base difference in sequence length\n",
      "     0.0275 std dev of per-base difference in sequence length\n"
     ]
    }
   ],
   "source": [
    "if not (args.mean and args.std):\n",
    "    print(\" - determining statistics about per-base difference ([align. subject len] - [align. query len]) / [align. query len] al from genome alignments\")\n",
    "    sequence_length_stats(gn_algn_df)\n",
    "    print(\"{:>11.4f} mean of per-base difference in sequence length\".format(args.mean))\n",
    "    print(\"{:>11.4f} std dev of per-base difference in sequence length\".format(args.std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - joining alignment data and filtering for adjacent adapter-genome alignments\n"
     ]
    }
   ],
   "source": [
    "print(\" - joining alignment data and filtering for adjacent adapter-genome alignments\")\n",
    "# determine order of alignments of each read with respect to each adapter-subject pair\n",
    "d = pd.concat([ad_algn_df, gn_algn_df], keys=['ad', 'gn'])\n",
    "for ad_id in adapter.index:\n",
    "    for gn_id in genome.index:\n",
    "        comb = [ad_id, gn_id]\n",
    "        o = d.loc[(d.subj.isin(comb))].sort_values(by=[\"qid\", \"qst\"], ascending=True)\n",
    "        d.loc[o.index, \":\".join(comb)] = np.arange(len(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ad = pd.merge(pd.DataFrame(reads.index, columns=['rid']).set_index('rid', drop=False), d.loc[d.index.droplevel(1) == 'ad'].set_index('qid', drop=False),\n",
    "                how='outer', left_index=True, right_index=True, sort=False)\n",
    "#d_gn = pd.merge(df, d.loc[d.index.droplevel(1) == 'gn'].set_index('qid', drop=False),\n",
    "#                how='outer', left_index=True, right_index=True, sort=False)\n",
    "df = pd.merge(d_ad, \n",
    "              d.loc[d.index.droplevel(1) == 'gn'].set_index('qid', drop=False), \n",
    "              how='outer', left_index=True, right_index=True, sort=False, suffixes=('_ad', '_gn'))\n",
    "# reset index\n",
    "df = df.reset_index(drop=True)\n",
    "sel = df.subj_ad.notnull() & \\\n",
    "      df.subj_gn.notnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     295580         entities after joining\n",
      "     246797  83.5 % not aligning against both an adapter and a genomic seq\n",
      "       7149   2.4 % are entries are alignments that are not adjacent to each other\n",
      "\n",
      "      37564  13.8 % reads remaining that contain potential transitions between adapter and genomic seq.\n",
      "      41634  14.1 % potential transitions remaining\n"
     ]
    }
   ],
   "source": [
    "# determine if the adapter alignment and the genome alignment are adjacent to each other with respect to \n",
    "# all other alignments of the given read to the same adapter and genome sequence\n",
    "for ad_id in adapter.index:\n",
    "    for gn_id in genome.index:\n",
    "        sel_ = (df.subj_ad == ad_id) & (df.subj_gn == gn_id)\n",
    "        df.loc[sel_, 'align_dist'] = (df[sel_][\"{}:{}_ad\".format(ad_id, gn_id)] - df[sel_][\"{}:{}_gn\".format(ad_id, gn_id)]).abs()\n",
    "print('{:>11} {:>7} entities after joining'.format(len(df), \"\"))\n",
    "c = sum(np.logical_not(sel))\n",
    "print('{:>11} {:>5.1f} % not aligning against both an adapter and a genomic seq'.format(c, c/len(df)*100.))\n",
    "c = sum((df.align_dist > 1.))\n",
    "print('{:>11} {:>5.1f} % are entries are alignments that are not adjacent to each other'.format(c, c/len(df)*100.))\n",
    "sel &= (df.align_dist == 1.)\n",
    "print()\n",
    "c = len(set(df.loc[sel, 'rid']))\n",
    "print('{:>11} {:>5.1f} % reads remaining that contain potential transitions between adapter and genomic seq.'.format(c, c/len(reads)*100.))\n",
    "c = sum(sel)\n",
    "print('{:>11} {:>5.1f} % potential transitions remaining'.format(c, c/len(df)*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - deleting all entries from dataframe that are not potential transitions\n"
     ]
    }
   ],
   "source": [
    "# deleting non-potential transitions at this point so that all procentual values are with respect to potential ones only\n",
    "print(\" - deleting all entries from dataframe that are not potential transitions\")\n",
    "df = df.drop(df.index[np.logical_not(sel)]).astype({\"qlen_ad\":np.int32, \"qst_ad\":np.int32, \"qen_ad\":np.int32, \"slen_ad\":np.int32,\n",
    "                                                   \"sst_ad\":np.int32, \"sen_ad\":np.int32, \"mlen_ad\":np.int32, \"blen_ad\":np.int32, \"mapq_ad\":np.int32,\n",
    "                                                   \"qlen_gn\":np.int32, \"qst_gn\":np.int32, \"qen_gn\":np.int32, \"slen_gn\":np.int32,\n",
    "                                                   \"sst_gn\":np.int32, \"sen_gn\":np.int32, \"mlen_gn\":np.int32, \"blen_gn\":np.int32, \"mapq_gn\":np.int32})\n",
    "sel = df.subj_ad.notnull() & \\\n",
    "      df.subj_gn.notnull() & \\\n",
    "      (df.align_dist == 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      24639  59.2 % of total potential transpositions are adapter -> genome\n",
      "      16995  40.8 % of total potential transpositions are genome -> adapter\n",
      " - filter potential subject transitions based on alignment lengths\n",
      "        231   0.6 % of total potential transpositions filtered\n",
      "         50   0.1 % due to insufficiently long adapter alignment\n",
      "        181   0.4 % due to insufficiently long genome alignment\n",
      "      41403  99.4 % potential transitions remaining\n"
     ]
    }
   ],
   "source": [
    "# identify if row describes a transition from adapter to genomic sequence or vise versa\n",
    "df.loc[sel,'trans_order'] = 0 # qst_ad == qst_gn\n",
    "df.loc[sel & (df.qst_ad < df.qst_gn), 'trans_order'] = 1 # adapter -> genome\n",
    "df.loc[sel & (df.qst_ad > df.qst_gn), 'trans_order'] = -1 # genome -> adapter\n",
    "c = sum(df[sel].qst_ad < df[sel].qst_gn)\n",
    "print('{:>11} {:>5.1f} % of total potential transpositions are adapter -> genome'.format(c, c/len(df[sel])*100.))\n",
    "c = sum(df[sel].qst_ad > df[sel].qst_gn)\n",
    "print('{:>11} {:>5.1f} % of total potential transpositions are genome -> adapter'.format(c, c/len(df[sel])*100.))\n",
    "\n",
    "# select all entries with an sufficiently long alignment to both the adapter and the genome\n",
    "print(' - filter potential subject transitions based on alignment lengths')\n",
    "sel_ = (df.blen_ad >= args.min_adapter_blen) & \\\n",
    "       (df.blen_gn >= args.min_genome_blen)\n",
    "c = sum(sel & np.logical_not(sel_))\n",
    "print('{:>11} {:>5.1f} % of total potential transpositions filtered'.format(c, c/len(df)*100.))\n",
    "c = sum(sel & np.logical_not(df.blen_ad >= args.min_adapter_blen))\n",
    "print('{:>11} {:>5.1f} % due to insufficiently long adapter alignment'.format(c, c/len(df)*100.))\n",
    "c = sum(sel & np.logical_not(df.blen_gn >= args.min_genome_blen))\n",
    "print('{:>11} {:>5.1f} % due to insufficiently long genome alignment'.format(c, c/len(df)*100.))\n",
    "sel &= sel_\n",
    "c = sum(sel)\n",
    "print('{:>11} {:>5.1f} % potential transitions remaining'.format(c, c/len(df)*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - determine query seq start and end in read coordinates\n",
      "       6829  16.5 % of remaining potential transpositions had < 6 matching terminal bases\n",
      "      34574  83.0 % potential transitions remaining\n"
     ]
    }
   ],
   "source": [
    "# determine query seq start and end in read coordinates\n",
    "print(\" - determine query seq start and end in read coordinates\")\n",
    "df = get_query_seq(df, sel)\n",
    "c = sum(sel & (df.query_st.isnull() | df.query_en.isnull()))\n",
    "print('{:>11} {:>5.1f} % of remaining potential transpositions had < {} matching terminal bases'.format(c, c/sum(sel)*100., args.wordsize))\n",
    "sel &= df.query_st.notnull() & df.query_en.notnull()\n",
    "c = sum(sel)\n",
    "print('{:>11} {:>5.1f} % potential transitions remaining'.format(c, c/len(df)*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - exclude entries based on query sequence length (adapter-genome alignment distance) from analysis\n",
      "        715   2.1 % of remaining potential transpositions removed due to query seq. length < 0 nt (alignment overlap of more than 5 nt)\n",
      "       1322   3.8 % of remaining potential transpositions removed due to query seq. length > 100 nt\n",
      "      32537  78.2 % potential transitions remaining\n"
     ]
    }
   ],
   "source": [
    "# filter out entries with query seq. that are too long -> distance between adapter and genome alignment is too large\n",
    "print(\" - exclude entries based on query sequence length (adapter-genome alignment distance) from analysis\")\n",
    "too_short = (df.query_en - df.query_st) < 0.\n",
    "too_long = (df.query_en - df.query_st) > args.max_dist\n",
    "c = sum(too_short)\n",
    "print('{:>11} {:>5.1f} % of remaining potential transpositions removed due to query seq. length < 0 nt (alignment overlap of more than {} nt)'.format(c, c/sum(sel)*100., args.strip))\n",
    "c = sum(too_long)\n",
    "print('{:>11} {:>5.1f} % of remaining potential transpositions removed due to query seq. length > {} nt'.format(c, c/sum(sel)*100., args.max_dist))\n",
    "sel &= np.logical_not(too_short) & np.logical_not(too_long)\n",
    "c = sum(sel)\n",
    "print('{:>11} {:>5.1f} % potential transitions remaining'.format(c, c/len(df)*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine min and max reference sequence length\n",
    "print(\" - determining possible reference sequence lengths\")\n",
    "df.loc[sel, 'min_ref_len'] = ((df[sel].query_en - df[sel].query_st) * (1 + args.mean - 3 * args.std) - 0.5).round()\n",
    "df.loc[sel, 'max_ref_len'] = ((df[sel].query_en - df[sel].query_st) * (1 + args.mean + 3 * args.std) + 0.5).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[sel, [\"rid\", \"trans_order\", \"strand_ad\", \"qst_ad\", \"qen_ad\", \"sst_ad\", \"sen_ad\", \"strand_gn\", \"qst_gn\", \"qen_gn\", \"sst_gn\", \"sen_gn\", \"query_st\", \"query_en\", \"min_ref_len\", \"max_ref_len\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adapter.index)\n",
    "print(adapter.loc['pUC19_ISCg1'].str[1628:1871+5].str[-15:])\n",
    "#print(reads['00020b9e-e8e4-406f-9fda-f06e0fd4c585'][301:306])\n",
    "#print(genome['MIT52'][1359051:1359160+5][-15:].reverse_complement())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_iter_items(iterable):\n",
    "        counter = count()\n",
    "        deque(zip(iterable, counter), maxlen=0)  # (consume at C speed)\n",
    "        return next(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cigar_str(query, subj):\n",
    "    cs = []\n",
    "    for q,s in zip(query, subj):\n",
    "        if q == s:\n",
    "            cs.append('=')\n",
    "        elif q == '-':\n",
    "            cs.append('D')\n",
    "        elif s == '-':\n",
    "            cs.append('I')\n",
    "        else:\n",
    "            cs.append('X')\n",
    "    return \"\".join(cs)\n",
    "    #return \"\".join([\"{}{}\".format(count_iter_items(g), k) for k,g in groupby(cs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_align_output(bstr, mat=False):\n",
    "    ret = []\n",
    "    for line in bstr.decode('utf8').split('\\n\\n'):\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            query, subj, score = line.split('\\n')\n",
    "            ret.append((query, subj, int(score[7:]), cigar_str(query, subj)))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align(row, fn=None):\n",
    "    query_seq = str(reads[row.rid][int(row.query_st):int(row.query_en)])\n",
    "    results = {}\n",
    "    subj_seqs = subject_seqs(row)\n",
    "    if fn:\n",
    "        with open(fn, 'w') as f:\n",
    "            for subj_seq in subj_seqs:\n",
    "                print(\">query\\n{}\\n>subject\\n{}\\n\".format(query_seq, subj_seq), file=f)\n",
    "        result = subprocess.run(['/Users/markushaak/sciebo/active_projects/crossalign/seq-align/bin/needleman_wunsch', '--printscores', '--file', fn], stdout=subprocess.PIPE).stdout\n",
    "    else:\n",
    "        result = b''\n",
    "        for seq in subj_seqs:\n",
    "            result += subprocess.run(['/Users/markushaak/sciebo/active_projects/crossalign/seq-align/bin/needleman_wunsch', '--printscores', query_seq, subj_seq], stdout=subprocess.PIPE).stdout\n",
    "\n",
    "    for subj_seq, (score, cs) in zip(subj_seqs.keys(), parse_align_output(result)):\n",
    "        for ref_len, j in subj_seqs[subj_seq]:\n",
    "            print(ref_len, j, subj_seq, score, cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "align(df.loc[7], \"test.fastq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subject_seqs(row):\n",
    "    subj_seqs = {}\n",
    "    for ref_len in range(row.min_ref_len, row.max_ref_len+1):\n",
    "        for j in range(ref_len+1):\n",
    "            if row.trans_order == 1.:\n",
    "                r1 = adapter.loc[row.subj_ad]\n",
    "                r1_strand = row.strand_ad\n",
    "                r1_sst = row.sst_ad\n",
    "                r1_sen = row.sen_ad\n",
    "                r2 = genome.loc[row.subj_gn]\n",
    "                r2_strand = row.strand_gn\n",
    "                r2_sst = row.sst_gn\n",
    "                r2_sen = row.sen_gn\n",
    "            else:\n",
    "                r1 = genome.loc[row.subj_gn]\n",
    "                r1_strand = row.strand_gn\n",
    "                r1_sst = row.sst_gn\n",
    "                r1_sen = row.sen_gn\n",
    "                r2 = adapter.loc[row.subj_ad]\n",
    "                r2_strand = row.strand_ad\n",
    "                r2_sst = row.sst_ad\n",
    "                r2_sen = row.sen_ad\n",
    "            \n",
    "            subj_seq = Seq('')\n",
    "            if j > 0:\n",
    "                if r1_strand == '+':\n",
    "                    ref_st = r1_sen - args.strip\n",
    "                    subj_seq += r1[ref_st : ref_st + j]\n",
    "                else:\n",
    "                    ref_en = r1_sst + args.strip\n",
    "                    subj_seq += r1[ref_en - j : ref_en].reverse_complement()\n",
    "            if (ref_len - j) > 0:\n",
    "                if r2_strand == '+':\n",
    "                    ref_en = r2_sst + args.strip\n",
    "                    subj_seq += r2[ref_en - (ref_len - j) : ref_en]\n",
    "                else:\n",
    "                    ref_st = r2_sen - args.strip\n",
    "                    subj_seq += r2[ref_st : ref_st + (ref_len - j)].reverse_complement()\n",
    "            \n",
    "            #subj_seqs.append((ref_len, j, str(subj_seq)))\n",
    "            subj_seq = str(subj_seq)\n",
    "            if subj_seq in subj_seqs:\n",
    "                subj_seqs[subj_seq].append((ref_len, j))\n",
    "            else:\n",
    "                subj_seqs[subj_seq] = [(ref_len, j)]\n",
    "    return subj_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reference_seqs(row):\n",
    "    if row.trans_order == 1.:\n",
    "        if row.strand_ad == '+':\n",
    "            row['ref1'] = adapter.loc[row.subj_ad].seq[(row.sen_ad - args.strip) : (row.sen_ad - args.strip) + int(row.max_ref_len)]\n",
    "        else:\n",
    "            row['ref1'] = adapter.loc[row.subj_ad].seq[(row.sst_ad + args.strip) - int(row.max_ref_len) : (row.sst_ad + args.strip)].translate(compl)[::-1]\n",
    "        if row.strand_gn == '+':\n",
    "            row['ref2'] = genome.loc[row.subj_gn].seq[(row.sst_gn + args.strip) - int(row.max_ref_len) : (row.sst_gn + args.strip)]\n",
    "        else:\n",
    "            row['ref2'] = genome.loc[row.subj_gn].seq[(row.sen_gn - args.strip) : (row.sen_gn - args.strip) + int(row.max_ref_len)].translate(compl)[::-1]\n",
    "    else:\n",
    "        if row.strand_gn == '+':\n",
    "            row['ref1'] = genome.loc[row.subj_gn].seq[(row.sen_gn - args.strip) : (row.sen_gn - args.strip) + int(row.max_ref_len)]\n",
    "        else:\n",
    "            row['ref1'] = genome.loc[row.subj_gn].seq[(row.sst_gn + args.strip) - int(row.max_ref_len) : (row.sst_gn + args.strip)].translate(compl)[::-1]\n",
    "        if row.strand_ad == '+':\n",
    "            row['ref2'] = adapter.loc[row.subj_ad].seq[(row.sst_ad + args.strip) - int(row.max_ref_len) : (row.sst_ad + args.strip)]\n",
    "        else:\n",
    "            row['ref2'] = adapter.loc[row.subj_ad].seq[(row.sen_ad - args.strip) : (row.sen_ad - args.strip) + int(row.max_ref_len)].translate(compl)[::-1]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set query seq for each row\n",
    "df.loc[sel, 'query_seq'] = df[sel].apply(lambda row: reads.loc[row.rid].seq[int(row.query_st) : int(row.query_en)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop('ref1', 1)\n",
    "#df = df.drop('ref2', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit -n 1 -r 1\n",
    "#df.loc[sel, 'ref1'] = ''\n",
    "#df.loc[sel, 'ref2'] = ''\n",
    "#df.loc[sel] = df[sel].apply(lambda row: reference_seqs(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit -n 1 -r 1\n",
    "# set first and second reference seq\n",
    "sel_ = sel & (df.trans_order == 1.) & (df.strand_ad == '+')\n",
    "df.loc[sel_, 'ref1'] = df[sel_].apply(\n",
    "    lambda row: adapter.loc[row.subj_ad].seq[(row.sen_ad - args.strip) : (row.sen_ad - args.strip) + int(row.max_ref_len)], axis=1)\n",
    "sel_ = sel & (df.trans_order == 1.) & (df.strand_ad == '-')\n",
    "df.loc[sel_, 'ref1'] = df[sel_].apply(\n",
    "    lambda row: adapter.loc[row.subj_ad].seq[(row.sst_ad + args.strip) - int(row.max_ref_len) : (row.sst_ad + args.strip)], axis=1)\n",
    "df.loc[sel_, 'ref1'] = df.loc[sel_, 'ref1'].str.translate(compl).str[::-1] # reverse complement\n",
    "\n",
    "\n",
    "sel_ = sel & (df.trans_order == 1.) & (df.strand_gn == '+')\n",
    "df.loc[sel_, 'ref2'] = df[sel_].apply(\n",
    "    lambda row: genome.loc[row.subj_gn].seq[(row.sst_gn + args.strip) - int(row.max_ref_len) : (row.sst_gn + args.strip)], axis=1)\n",
    "sel_ = sel & (df.trans_order == 1.) & (df.strand_gn == '-')\n",
    "df.loc[sel_, 'ref2'] = df[sel_].apply(\n",
    "    lambda row: genome.loc[row.subj_gn].seq[(row.sen_gn - args.strip) : (row.sen_gn - args.strip) + int(row.max_ref_len)], axis=1)\n",
    "df.loc[sel_, 'ref2'] = df.loc[sel_, 'ref2'].str.translate(compl).str[::-1] # reverse complement\n",
    "\n",
    "\n",
    "sel_ = sel & (df.trans_order == -1.) & (df.strand_gn == '+')\n",
    "df.loc[sel_, 'ref1'] = df[sel_].apply(\n",
    "    lambda row: genome.loc[row.subj_gn].seq[(row.sen_gn - args.strip) : (row.sen_gn - args.strip) + int(row.max_ref_len)], axis=1)\n",
    "sel_ = sel & (df.trans_order == -1.) & (df.strand_gn == '-')\n",
    "df.loc[sel_, 'ref1'] = df[sel_].apply(\n",
    "    lambda row: genome.loc[row.subj_gn].seq[(row.sst_gn + args.strip) - int(row.max_ref_len) : (row.sst_gn + args.strip)], axis=1)\n",
    "df.loc[sel_, 'ref1'] = df.loc[sel_, 'ref1'].str.translate(compl).str[::-1] # reverse complement\n",
    "\n",
    "\n",
    "sel_ = sel & (df.trans_order == -1.) & (df.strand_ad == '+')\n",
    "df.loc[sel_, 'ref2'] = df[sel_].apply(\n",
    "    lambda row: adapter.loc[row.subj_ad].seq[(row.sst_ad + args.strip) - int(row.max_ref_len) : (row.sst_ad + args.strip)], axis=1)\n",
    "sel_ = sel & (df.trans_order == -1.) & (df.strand_ad == '-')\n",
    "df.loc[sel_, 'ref2'] = df[sel_].apply(\n",
    "    lambda row: adapter.loc[row.subj_ad].seq[(row.sen_ad - args.strip) : (row.sen_ad - args.strip) + int(row.max_ref_len)], axis=1)\n",
    "df.loc[sel_, 'ref2'] = df.loc[sel_, 'ref2'].str.translate(compl).str[::-1] # reverse complement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[sel, ['trans_order', 'strand_ad', 'strand_gn', 'query_seq', 'ref1', 'ref2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tables(s, lq, ls):\n",
    "    lq += 1\n",
    "    ls += 1\n",
    "    sio = StringIO(s.decode('utf8'))\n",
    "    return pd.read_table(sio, skiprows=[0,1,2,2+(ls+1)*1,2+(ls+1)*2], nrows=ls*3, usecols=range(1,lq+1), header=None).to_numpy().reshape((3,ls,lq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_single(query, subject):\n",
    "    cmd = ['/Users/markushaak/sciebo/active_projects/crossalign/seq-align/bin/needleman_wunsch', \n",
    "           '--printscores', '--printmatrices', '--match', '1', '--mismatch', '-2', '--gapopen', '-4', '--gapextend', '-1',\n",
    "           query, subject]\n",
    "    res = subprocess.run(cmd, stdout=subprocess.PIPE).stdout\n",
    "    return parse_tables(res, len(query), len(subject))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtrace_css(ops, cs=[]):\n",
    "    traces = []\n",
    "    if ops.size <= 4:\n",
    "        #return [\"\".join([\"{}{}\".format(count_iter_items(g), k) for k,g in groupby(cs)])]\n",
    "        return [\"\".join(cs)]\n",
    "    if ops[0,-1,-1]: # insertion\n",
    "        traces += ( backtrace_css(ops[:,:,:-1], ['I'] + cs) )\n",
    "    if ops[1,-1,-1]: # deletion\n",
    "        traces += ( backtrace_css(ops[:,:-1,:], ['D'] + cs) )\n",
    "    if ops[2,-1,-1] and ops[3,-1,-1]: # match/mismatch both seqs\n",
    "        traces += ( backtrace_css(ops[:,:-1,:-1], ['='] + cs) )\n",
    "    elif ops[2,-1,-1]: # match/mismatch first seq\n",
    "        traces += ( backtrace_css(ops[:,:-1,:-1], ['L'] + cs) )\n",
    "    elif ops[3,-1,-1]: # match/mismatch second seq\n",
    "        traces += ( backtrace_css(ops[:,:-1,:-1], ['R'] + cs) )\n",
    "    return traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtrace_css_gapped(ops, s1len, cs=[]):\n",
    "    traces = []\n",
    "    if sum(ops.shape[1:]) == 2:\n",
    "        #return [\"\".join([\"{}{}\".format(count_iter_items(g), k) for k,g in groupby(cs)])]\n",
    "        return [\"\".join(cs)]\n",
    "    if ops[0,-1,-1]: # insertion\n",
    "        traces += ( backtrace_css_gapped(ops[:,:,:-1], s1len, ['I'] + cs) )\n",
    "    if ops[1,-1,-1]: # deletion\n",
    "        traces += ( backtrace_css_gapped(ops[:,:-1,:], s1len, ['D'] + cs) )\n",
    "    if ops[2,-1,-1]: # match/mismatch both seqs\n",
    "        traces += ( backtrace_css_gapped(ops[:,:-1,:-1], s1len, ['='] + cs) )\n",
    "    if ops[3,-1,-1]: # free gap\n",
    "        if ops.shape[1] > s1len+1:\n",
    "            jump_to = ops[4,s1len+1:,-1].argmax() + s1len+1 + 1\n",
    "            char = '>'\n",
    "        else:\n",
    "            jump_to = ops[4,:,-1].argmax() + 1\n",
    "            char = '<'\n",
    "        stepsize = ops.shape[1] - jump_to\n",
    "        traces += ( backtrace_css_gapped(ops[:,:jump_to,:], s1len, [char]*stepsize + cs) )\n",
    "    return traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtrace_tps(ops, s2_start=None):\n",
    "    tps = []\n",
    "    if ops[0,-1,-1]: # insertion\n",
    "        tps += (backtrace_tps(ops[:,:,:-1], s2_start))\n",
    "    if ops[1,-1,-1]: # deletion\n",
    "        tps += (backtrace_tps(ops[:,:-1,:], s2_start))\n",
    "    if ops[3,-1,-1]: # match/mismatch second seq\n",
    "        tps += (backtrace_tps(ops[:,:-1,:-1], ops.shape[2] - 1))\n",
    "    if ops[2,-1,-1]: # match/mismatch first seq\n",
    "        return tps + [(ops.shape[1] - 1, s2_start)]\n",
    "    return tps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(backtrace_tps(ops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(backtrace_css(ops, []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gapped_twinalign(query, subj1, subj2, m=[-2,1], go=-4, ge=-1):\n",
    "    qlen, s1len, s2len = len(query), len(subj1), len(subj2)\n",
    "    # stores operations (insertion, deletion, match/mismatch, endgap_start, endgap_end) that maximize the score\n",
    "    ops = np.zeros(shape=(5, s1len+s2len+1, qlen+1), dtype=np.bool_)\n",
    "    ops[2,0,0] = True\n",
    "    ops[0,0,1:] = True\n",
    "    ops[1,1:s1len,0] = True\n",
    "    ops[4,0,0] = True\n",
    "    ops[3,s1len+1:,0] = True\n",
    "    # stores the best partial alignment scores\n",
    "    scores = np.empty(shape=(s1len+s2len+1, qlen+1), dtype=np.int32)\n",
    "    scores[0,0] = 0\n",
    "    scores[0,1:] = np.arange(go + ge, go + (qlen+1)*ge, ge)\n",
    "    scores[1:s1len+1,0] = np.arange(go + ge, go + (s1len+1)*ge, ge)\n",
    "    \n",
    "    op_scores = np.empty(shape=(4,), dtype=np.int32)\n",
    "    for i in range(1,s1len):\n",
    "        for j in range(1,qlen+1):\n",
    "            # insertion\n",
    "            op_scores[0] = scores[i,j-1] + (not ops[0,i,j-1]) * go + ge\n",
    "            # deletion\n",
    "            op_scores[1] = scores[i-1,j] + (not ops[1,i-1,j]) * go + ge\n",
    "            # match/mismatch\n",
    "            op_scores[2] = scores[i-1,j-1] + m[subj1[i-1] == query[j-1]]\n",
    "            \n",
    "            scores[i,j] = op_scores[:3].max()\n",
    "            ops[:3,i,j] = op_scores[:3] == scores[i,j]\n",
    "    for i in range(s1len,s1len+1):\n",
    "        for j in range(1,qlen+1):\n",
    "            # insertion\n",
    "            op_scores[0] = scores[i,j-1] + (not ops[0,i,j-1]) * go + ge\n",
    "            # deletion\n",
    "            op_scores[1] = scores[i-1,j] + (not ops[1,i-1,j]) * go + ge\n",
    "            # match/mismatch\n",
    "            op_scores[2] = scores[i-1,j-1] + m[subj1[i-1] == query[j-1]]\n",
    "            # free end gap\n",
    "            max_in_col = scores[:i,j].argmax()\n",
    "            op_scores[3] = scores[max_in_col,j]\n",
    "            \n",
    "            scores[i,j] = op_scores.max()\n",
    "            ops[:3,i,j] = op_scores[:3] == scores[i,j]\n",
    "            if op_scores[3] == scores[i,j]:\n",
    "                ops[3,i,j] = True # gap start\n",
    "                ops[4,max_in_col,j] = True # gap end\n",
    "            else:\n",
    "                ops[4,i,j] = True # gap end\n",
    "    #scores[s1len+1,0] = 0\n",
    "    #scores[s1len+2:,0] = np.arange(go + ge, go + (s2len+1)*ge, ge)\n",
    "    scores[s1len:,0] = 0\n",
    "    scores[s1len:,0] = 0\n",
    "    for i in range(s1len+1,s1len+s2len+1):\n",
    "        for j in range(1,qlen+1):\n",
    "            # insertion\n",
    "            op_scores[0] = scores[i,j-1] + (not ops[0,i,j-1]) * go + ge\n",
    "            # deletion\n",
    "            op_scores[1] = scores[i-1,j] + (not ops[1,i-1,j]) * go + ge\n",
    "            # match/mismatch\n",
    "            op_scores[2] = scores[i-1,j-1] + m[subj2[i-s1len-1] == query[j-1]]\n",
    "            # free start gap\n",
    "            op_scores[3] = scores[s1len,j]\n",
    "            \n",
    "            scores[i,j] = op_scores.max()\n",
    "            ops[:4,i,j] = op_scores == scores[i,j]\n",
    "            if ops[3,i,j]:\n",
    "                ops[4,s1len,j] = True # gap end\n",
    "    return scores, ops#np.packbits(ops, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "row = df.loc[7]\n",
    "q, s1, s2 = \"ATCC\", \"ATTTTTTTT\", \"TCC\"\n",
    "#q, s1, s2 = row.query_seq, row.ref1, row.ref2\n",
    "score, ops = gapped_twinalign(q, s1, s2, m=[-4,1])\n",
    "print(score)\n",
    "#print(ops)\n",
    "print(q)\n",
    "print(s1+s2)\n",
    "#for cs in backtrace_css_gapped(ops, len(s1), []):\n",
    "#    print(cs)\n",
    "#set(backtrace_tps(ops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def twinalign(query, subj1, subj2, m=[-2,1], go=-4, ge=-1):\n",
    "    qlen, slen = len(query)+1, len(subj1)+1\n",
    "    # stores operations (insertion, deletion, match/mismatch with first and second subj. seq) that maximize the score\n",
    "    ops = np.zeros(shape=(4, slen, qlen), dtype=np.bool_)\n",
    "    switched = np.zeros(shape=(slen, qlen), dtype=np.bool_)\n",
    "    # stores the best partial alignment scores\n",
    "    scores = np.empty(shape=(slen, qlen), dtype=np.int32)\n",
    "    \n",
    "    scores[0,0] = 0\n",
    "    scores[0,1:] = np.arange(go + ge, go + qlen*ge, ge)\n",
    "    scores[1:,0] = np.arange(go + ge, go + slen*ge, ge)\n",
    "    \n",
    "    ops[2,0,0] = True\n",
    "    ops[0,0,1:] = True\n",
    "    ops[1,1:,0] = True\n",
    "    \n",
    "    op_scores = np.empty(shape=(4,), dtype=np.int32)\n",
    "    for i in range(1,slen):\n",
    "        for j in range(1,qlen):\n",
    "            # insertion\n",
    "            op_scores[0] = scores[i,j-1] + (not ops[0,i,j-1]) * go + ge\n",
    "            # deletion\n",
    "            op_scores[1] = scores[i-1,j] + (not ops[1,i-1,j]) * go + ge\n",
    "            # match/mismatch with first subject seq\n",
    "            if not switched[i-1, j-1]: #if not (sum(ops[:,i-1,j-1]) == 1 and ops[3,i-1,j-1]):\n",
    "                op_scores[2] = scores[i-1,j-1] + m[subj1[i-1] == query[j-1]]\n",
    "            else:\n",
    "                np.iinfo(scores.dtype).min\n",
    "            # match/mismatch with second subject seq\n",
    "            op_scores[3] = scores[i-1,j-1] + m[subj2[i-1] == query[j-1]]\n",
    "            \n",
    "            scores[i,j] = op_scores.max()\n",
    "            ops[:,i,j] = op_scores == scores[i,j]\n",
    "            \n",
    "            # set switched state\n",
    "            if (ops[0,i,j] and switched[i,j-1]) or \\\n",
    "               (ops[1,i,j] and switched[i-1,j]) or \\\n",
    "               (ops[3,i,j] and not ops[2,i,j]):\n",
    "                switched[i,j] = True\n",
    "        \n",
    "    return scores[slen-1,qlen-1], ops#np.packbits(ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def align_twice(row):\n",
    "    #query1, subj1, score1, cs1 = align_single(row.query_seq, row.ref1)\n",
    "    #query2, subj2, score2, cs2 = align_single(row.query_seq, row.ref2)\n",
    "    #query, subj, score, cs = align_single(query1, query2)\n",
    "    #\n",
    "    #print(\">adapter\\n{}\\n>read\\n{}\\n>genome\\n{}\\n\".format(row.ref1, row.query_seq, row.ref2))\n",
    "    #\n",
    "    #print(query1)\n",
    "    #print(cs1)\n",
    "    #print(subj1)\n",
    "    #print()\n",
    "    #print(query2)\n",
    "    #print(cs2)\n",
    "    #print(subj2)\n",
    "    #print()\n",
    "    #print(query)\n",
    "    #print(subj)\n",
    "    dp_tab1 = align_single(row.query_seq, row.ref1)\n",
    "    dp_tab2 = align_single(row.query_seq, row.ref2)\n",
    "\n",
    "    display(pd.DataFrame(np.amax(dp_tab1, axis=0), columns=list(\"-\"+row.query_seq), index=pd.Index(list(\"-\"+row.ref1))))\n",
    "    display(pd.DataFrame(np.argmax(dp_tab1, axis=0), columns=list(\"-\"+row.query_seq), index=pd.Index(list(\"-\"+row.ref1))))\n",
    "    #for t,name in [(0,\"match_scores\"), (1, \"gap_a_scores\"), (2, \"gap_b_scores\")]:\n",
    "    #    print(name)\n",
    "    #    display(pd.DataFrame(dp_tab1[t], columns=list(\"-\"+row.query_seq), index=pd.Index(list(\"-\"+row.ref1))))\n",
    "    #print(dp_tab2)\n",
    "    display(pd.DataFrame(np.amax(dp_tab2, axis=0), columns=list(\"-\"+row.query_seq), index=pd.Index(list(\"-\"+row.ref2))))\n",
    "    display(pd.DataFrame(np.argmax(dp_tab2, axis=0), columns=list(\"-\"+row.query_seq), index=pd.Index(list(\"-\"+row.ref2))))\n",
    "    \n",
    "align_twice(df.loc[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_align(query, ref1, ref2):\n",
    "    dp_tab1 = align_single(query, ref1)\n",
    "    dp_tab2 = align_single(query, ref2)\n",
    "\n",
    "    display(pd.DataFrame(np.amax(dp_tab1, axis=0), columns=list(\"-\"+query), index=pd.Index(list(\"-\"+ref1))))\n",
    "    display(pd.DataFrame(np.argmax(dp_tab1, axis=0), columns=list(\"-\"+query), index=pd.Index(list(\"-\"+ref1))))\n",
    "    for t,name in [(0,\"match_scores\"), (1, \"gap_a_scores\"), (2, \"gap_b_scores\")]:\n",
    "        print(name)\n",
    "        display(pd.DataFrame(dp_tab1[t], columns=list(\"-\"+query), index=pd.Index(list(\"-\"+ref1))))\n",
    "\n",
    "    display(pd.DataFrame(np.amax(dp_tab2, axis=0), columns=list(\"-\"+query), index=pd.Index(list(\"-\"+ref2))))\n",
    "    display(pd.DataFrame(np.argmax(dp_tab2, axis=0), columns=list(\"-\"+query), index=pd.Index(list(\"-\"+ref2))))\n",
    "    for t,name in [(0,\"match_scores\"), (1, \"gap_a_scores\"), (2, \"gap_b_scores\")]:\n",
    "        print(name)\n",
    "        display(pd.DataFrame(dp_tab2[t], columns=list(\"-\"+query), index=pd.Index(list(\"-\"+ref2))))\n",
    "    \n",
    "\n",
    "test_align(\"ATC\", \"ATG\", \"CTC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get reference sequences of length ref_len with transition at j\n",
    "ref_len = 6\n",
    "trans = 3\n",
    "loc_ = (df.min_ref_len <= 5) & (df.max_ref_len >= 5)\n",
    "df[loc_].ref1.str[:j] + df[loc_].ref2.str[j-ref_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = df[sel][['query_seq', 'min_ref_len', 'max_ref_len', 'ref1', 'ref2']].astype({'min_ref_len':np.int32, 'max_ref_len':np.int32})\n",
    "d['ref_len'] = d.apply(lambda row: list(range(row.min_ref_len, row.max_ref_len+1)), axis=1)\n",
    "d = d.explode('ref_len')\n",
    "d['trans'] = d.apply(lambda row: list(range(row.ref_len+1)), axis=1)\n",
    "d = d.explode('trans')\n",
    "#d.ref1.str[:d.trans] + d.ref2.str[d.trans-d.ref_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "d['subject_seq'] = ''\n",
    "d['subject_seq'] = d.apply(lambda row: row.ref1[:row.trans] + row.ref2[row.max_ref_len-row.ref_len+row.trans:], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "d['subject_seq'] = ''\n",
    "for ref_len in range(d.ref_len.min(), d.ref_len.max()+1):\n",
    "    sel__ = (d.ref_len == ref_len)\n",
    "    for trans in range(ref_len+1):\n",
    "        sel_ = sel__ & (d.trans == trans)\n",
    "        d.loc[sel_, 'subject_seq'] = df[loc_].ref1.str[:trans] + df[loc_].ref2.str[trans-ref_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[['query_seq', 'subject_seq']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.loc[(d[['subject_seq']].shift(-1) != d[['subject_seq']]).subject_seq, ['query_seq', 'subject_seq']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#row_queries = {} # stores the queries for each potential transition together with the \n",
    "queries = {} # stores all non-redundant queries\n",
    "for i,row in tqdm(df[sel].astype({'query_st':np.int32, 'query_en':np.int32}).iterrows()):\n",
    "    # determine the query seq\n",
    "    query_seq = reads[row.rid][row.query_st : row.query_en]\n",
    "    if query_seq not in queries:\n",
    "        queries[query_seq] = {}\n",
    "    # determine the corresponding subject seqs\n",
    "    for req, subj_seq in subject_seqs(df, i):\n",
    "        if subj_seq not in queries[query_seq]:\n",
    "            queries[query_seq][subj_seq] = [req]\n",
    "        else:\n",
    "            queries[query_seq][subj_seq].append(req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_algn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
