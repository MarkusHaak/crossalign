{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandarallel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a6eaa45097ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mctypes\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandarallel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandarallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandarallel'"
     ]
    }
   ],
   "source": [
    "import os, argparse, shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "from itertools import groupby, count\n",
    "from collections import deque\n",
    "from io import StringIO\n",
    "import ctypes as ct\n",
    "import re\n",
    "from pandarallel import pandarallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate for building complement of a DNA sequence\n",
    "compl = str.maketrans('ATGCNatgcn', 'TACGNatgcn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgHelpFormatter(argparse.HelpFormatter):\n",
    "    '''\n",
    "    Formatter adding default values to help texts.\n",
    "    '''\n",
    "    def __init__(self, prog):\n",
    "        super().__init__(prog)\n",
    "\n",
    "    ## https://stackoverflow.com/questions/3853722\n",
    "    #def _split_lines(self, text, width):\n",
    "    #   if text.startswith('R|'):\n",
    "    #       return text[2:].splitlines()  \n",
    "    #   # this is the RawTextHelpFormatter._split_lines\n",
    "    #   return argparse.HelpFormatter._split_lines(self, text, width)\n",
    "\n",
    "    def _get_help_string(self, action):\n",
    "        text = action.help\n",
    "        if  action.default is not None and \\\n",
    "            action.default != argparse.SUPPRESS and \\\n",
    "            'default:' not in text.lower():\n",
    "            text += ' (default: {})'.format(action.default)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args(args=None):\n",
    "    parser = argparse.ArgumentParser(description='Estimates read starts (transposase insertion sites) for ONT rapid libraries',\n",
    "                                     formatter_class=ArgHelpFormatter, \n",
    "                                     add_help=False)\n",
    "\n",
    "    main_group = parser.add_argument_group('Main Options')\n",
    "    main_group.add_argument('reads',\n",
    "                            nargs='+',\n",
    "                            help='fastq files or path to directories containing fastq files (recursion depth 1)')\n",
    "    main_group.add_argument('genome',\n",
    "                            help='Fasta file containing the genomic sequences that is searched for insertion sites.')\n",
    "    main_group.add_argument('adapter',\n",
    "                            help='Transposon Y adapter sequence')\n",
    "    main_group.add_argument('--prefix',\n",
    "                            help=\"filename for readstarts in tap seperated value (.tsv) format\",\n",
    "                            default=\"readstarts\")\n",
    "    #main_group.add_argument('--verbose_out_file',\n",
    "    #                        help=\"print detailed, human readable information about each read to this file\")\n",
    "    #main_group.add_argument('--verbose_col_width',\n",
    "    #                        help=\"column width for verbose output\",\n",
    "    #                        type=int,\n",
    "    #                        default=250)\n",
    "    main_group.add_argument('--plot',\n",
    "                            help='plot results of gaussian approximation',\n",
    "                            action='store_true')\n",
    "    main_group.add_argument('--circular',\n",
    "                            action=\"store_true\")\n",
    "    main_group.add_argument('--strip',\n",
    "                            help=\"number of bases stripped from alignments to cope with coincidently identical sequences\",\n",
    "                            type=int,\n",
    "                            default=5)\n",
    "    main_group.add_argument('--wordsize',\n",
    "                            help='',\n",
    "                            type=int,\n",
    "                            default=8)\n",
    "    main_group.add_argument('--max_dist',\n",
    "                            help='''max distance between an adapter and a genome alignment to perform pairwise-alignment \n",
    "                                 in order to identify the exact transition point''',\n",
    "                            type=int,\n",
    "                            default=200)\n",
    "    main_group.add_argument('--min_readlength',\n",
    "                            help=\"min length of reads to be analyzed\",\n",
    "                            type=int,\n",
    "                            default=500)\n",
    "    main_group.add_argument('--processes',\n",
    "                            type=int,\n",
    "                            default=6)\n",
    "    main_group.add_argument('--batchsize',\n",
    "                            type=int,\n",
    "                            default=8000)\n",
    "    \n",
    "    align_group = parser.add_argument_group('Alignment Options')\n",
    "    align_group.add_argument('--match',\n",
    "                             help='match score',\n",
    "                             type=int,\n",
    "                             default=1)\n",
    "    align_group.add_argument('--mismatch',\n",
    "                             help='mismatch penalty',\n",
    "                             type=int,\n",
    "                             default=-2)\n",
    "    align_group.add_argument('--gap_open',\n",
    "                             help='gap open penalty',\n",
    "                             type=int,\n",
    "                             default=-4)\n",
    "    align_group.add_argument('--gap_extension',\n",
    "                             help='gap extension penalty',\n",
    "                             type=int,\n",
    "                             default=-1)\n",
    "\n",
    "    filter_group = parser.add_argument_group('Filter Options')\n",
    "    filter_group.add_argument('--mean',\n",
    "                              help='mean of per-base difference of actual sequence length from read length',\n",
    "                              type=float)\n",
    "    filter_group.add_argument('--std',\n",
    "                              help='standard deviation of per-base difference of actual sequence length from read length',\n",
    "                              type=float)\n",
    "    filter_group.add_argument('--min_blen',\n",
    "                              help=\"min produced alignment length (including errors)\",\n",
    "                              type=int,\n",
    "                              default=20)\n",
    "    filter_group.add_argument('--min_adapter_blen',\n",
    "                              help=\"min produced alignment length (including errors)\",\n",
    "                              type=int,\n",
    "                              default=50)\n",
    "    filter_group.add_argument('--min_genome_blen',\n",
    "                              help=\"min produced alignment length (including errors)\",\n",
    "                              type=int,\n",
    "                              default=50)\n",
    "    filter_group.add_argument('--f_window',\n",
    "                              help=\"sequence window around the position of transition from \"+\\\n",
    "                                   \"the adapter sequence to the chromosome sequence\",\n",
    "                              type=int,\n",
    "                              default=7)\n",
    "    filter_group.add_argument('--f_max_w_err',\n",
    "                              help=\"max amount of errors (insertions, deletions, mismatches) \"+\\\n",
    "                                   \"within the specified sequence window\",\n",
    "                              type=int,\n",
    "                              default=3)\n",
    "    filter_group.add_argument('--f_max_mm_strech',\n",
    "                              help=\"max amount of deletions or insertions in the whole realignment\",\n",
    "                              type=int,\n",
    "                              default=3)\n",
    "\n",
    "    help_group = parser.add_argument_group('Help')\n",
    "    help_group.add_argument('-h', '--help', \n",
    "                            action='help', \n",
    "                            default=argparse.SUPPRESS,\n",
    "                            help='Show this help message and exit.')\n",
    "    if args:\n",
    "        return parser.parse_args(args)\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_minimap2(ref_fn, fq_fn, paf_fn):\n",
    "    cmd ='minimap2 -x map-ont -c --eqx --secondary=no -t 4 {} {} >{} 2> /dev/null'.format(ref_fn, fq_fn, paf_fn)\n",
    "    #print('Running:', cmd)\n",
    "    return os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_paf(fn, cigar=False):\n",
    "    usecols = list(range(12))\n",
    "    names = [\"qid\", \"qlen\", \"qst\", \"qen\", \"strand\", \"subj\", \n",
    "             \"slen\", \"sst\", \"sen\", \"mlen\", \"blen\", \"mapq\"]\n",
    "    dtype = {\"qid\": str, \"qlen\": np.int32, \"qst\": np.int32, \n",
    "             \"qen\": np.int32, \"strand\": str, \"subj\": str,\n",
    "             \"slen\": np.int32, \"sst\": np.int32, \"sen\": np.int32, \n",
    "             \"mlen\": np.int32, \"blen\": np.int32, \"mapq\": np.int32}\n",
    "    converters = {}\n",
    "    if cigar:\n",
    "        usecols.append(22)\n",
    "        names.append('cg')\n",
    "        converters['cg'] = lambda x: x.split(':')[-1]\n",
    "    return pd.read_csv(fn, sep='\\t', header=None,\n",
    "                       usecols=usecols,\n",
    "                       names=names,\n",
    "                       dtype=dtype,\n",
    "                       converters=converters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_query_st(row):\n",
    "    trimmed_query = 0\n",
    "    trimmed_subject = 0\n",
    "    if row.trans_order == 1.:\n",
    "        cg = row.cg_ad\n",
    "        qen = row.qen_ad\n",
    "    else:\n",
    "        cg = row.cg_gn\n",
    "        qen = row.qen_gn\n",
    "    cg_list = re.findall(cg_pat, cg)\n",
    "    for bases,op in reversed(cg_list):\n",
    "        bases = int(bases)\n",
    "        if op == \"=\" and bases >= args.wordsize:\n",
    "            return qen - trimmed_query, trimmed_subject\n",
    "            break\n",
    "        if op == \"=\" or op == 'X':\n",
    "            trimmed_query += bases\n",
    "            trimmed_subject += bases\n",
    "        elif op == 'I':\n",
    "            trimmed_query += bases\n",
    "        else: # == D\n",
    "            trimmed_subject += bases\n",
    "    # unsuccessful\n",
    "    return np.nan, np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_query_en(row):\n",
    "    trimmed_query = 0\n",
    "    trimmed_subject = 0\n",
    "    if row.trans_order == 1.:\n",
    "        cg = row.cg_gn\n",
    "        qst = row.qst_gn\n",
    "    else:\n",
    "        cg = row.cg_ad\n",
    "        qst = row.qst_ad\n",
    "    for m in re.finditer(cg_pat, cg):\n",
    "        bases, op = int(m.group(1)), m.group(2)\n",
    "        if op == \"=\" and bases >= args.wordsize:\n",
    "            return qst + trimmed_query, trimmed_subject\n",
    "            break\n",
    "        if op == \"=\" or op == 'X':\n",
    "            trimmed_query += bases\n",
    "            trimmed_subject += bases\n",
    "        elif op == 'I':\n",
    "            trimmed_query += bases\n",
    "        else: # == D\n",
    "            trimmed_subject += bases\n",
    "    # unsuccessful\n",
    "    return np.nan, np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_seq(df, sel, fix=True):\n",
    "    df['ref1_trim'], df['ref2_trim'] = 0, 0\n",
    "    \n",
    "    cg_ad = df.cg_ad.str.replace('D|I', 'X')\n",
    "    cg_gn = df.cg_gn.str.replace('D|I', 'X')\n",
    "    \n",
    "    sel_ = sel & (df.trans_order == 1.) & (cg_ad.str.rstrip('=').str.rsplit('X', n=1).str[-1].astype(np.float32) >= args.wordsize) # & (df.strand_ad == '+')\n",
    "    df.loc[sel_ , 'query_st'] = df[sel_].qen_ad\n",
    "    sel_ = sel & (df.trans_order == 1.) & (cg_gn.str.split('=', n=1).str[0].astype(np.float32) >= args.wordsize) # & (df.strand_ad == '+')\n",
    "    df.loc[sel_ , 'query_en'] = df[sel_].qst_gn\n",
    "    \n",
    "    sel_ = sel & (df.trans_order == -1.) & (cg_gn.str.rstrip('=').str.rsplit('X', n=1).str[-1].astype(np.float32) >= args.wordsize) # & (df.strand_ad == '+') \n",
    "    df.loc[sel_ , 'query_st'] = df[sel_].qen_gn\n",
    "    sel_ = sel & (df.trans_order == -1.) & (cg_ad.str.split('=', n=1).str[0].astype(np.float32) >= args.wordsize) # & (df.strand_ad == '+')\n",
    "    df.loc[sel_ , 'query_en'] = df[sel_].qst_ad\n",
    "    \n",
    "    if fix:\n",
    "        # fix query end & start of reads with insufficient terminal alignments\n",
    "        df.loc[sel & df.query_st.isnull(), ['query_st', 'ref1_trim']] = pd.DataFrame(\n",
    "            df.loc[sel & df.query_st.isnull()].apply(lambda row: fix_query_st(row), axis=1).values.tolist(), \n",
    "            index=df.loc[sel & df.query_st.isnull()].index, columns=['query_st', 'ref1_trim']\n",
    "        )\n",
    "        df.loc[sel & df.query_en.isnull(), ['query_en', 'ref2_trim']] = pd.DataFrame(\n",
    "            df.loc[sel & df.query_en.isnull()].apply(lambda row: fix_query_en(row), axis=1).values.tolist(), \n",
    "            index=df.loc[sel & df.query_en.isnull()].index, columns=['query_en', 'ref2_trim']\n",
    "        )\n",
    "    \n",
    "    df.loc[sel, 'query_st'] -= args.strip\n",
    "    df.loc[sel, 'query_en'] += args.strip\n",
    "    df.loc[sel, 'ref1_trim'] += args.strip\n",
    "    df.loc[sel, 'ref2_trim'] += args.strip\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_length_stats(df):\n",
    "    dfs = []\n",
    "    # add length of read seq that aligns as \"qalen\"\n",
    "    dfs.append((df[df.subj.notnull()].qen - df[df.subj.notnull()].qst).to_frame(name='qalen'))\n",
    "    # add length of genome seq that aligns as \"salen\"\n",
    "    dfs.append((df[df.subj.notnull()].sen - df[df.subj.notnull()].sst).abs().to_frame(name='salen'))\n",
    "    ## add match, mismatch, insertion and deletion counts\n",
    "    #two_groups = '(?P<digit>[0-9]*)(?P<letter>[=XID])'\n",
    "    #d = df[df.subj_gn.notnull()].cg_gn.str.extractall(two_groups)\n",
    "    #d.loc[:, 'digit'] = d.digit.astype(\"float32\")\n",
    "    #for letter in ['=', 'X', 'I', 'D']:\n",
    "    #    dfs.append(d.loc[d.letter == letter].groupby(level=0).agg('sum').rename(columns = {'digit':letter}))\n",
    "    stats = pd.concat(dfs, axis=1).fillna(0.)\n",
    "    data = (stats.salen - stats.qalen)/stats.qalen\n",
    "    args.mean, args.std = norm.fit(data)\n",
    "    if args.plot:\n",
    "        fig, ax = plt.subplots(figsize=(8,6))\n",
    "        ax.hist(data, bins=100, density=True)\n",
    "        xmin, xmax = ax.get_xlim()\n",
    "        x = np.linspace(xmin, xmax, 200)\n",
    "        y = norm.pdf(x, args.mean, args.std)\n",
    "        ax.plot(x, y)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_iter_items(iterable):\n",
    "        counter = count()\n",
    "        deque(zip(iterable, counter), maxlen=0)  # (consume at C speed)\n",
    "        return next(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cigar_str(query, subj):\n",
    "    cs = []\n",
    "    for q,s in zip(query, subj):\n",
    "        if q == s:\n",
    "            cs.append('=')\n",
    "        elif q == '-':\n",
    "            cs.append('D')\n",
    "        elif s == '-':\n",
    "            cs.append('I')\n",
    "        else:\n",
    "            cs.append('X')\n",
    "    return \"\".join(cs)\n",
    "    #return \"\".join([\"{}{}\".format(count_iter_items(g), k) for k,g in groupby(cs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtrace_css_gapped(ops, s1len, cs=[]):\n",
    "    traces = []\n",
    "    if sum(ops.shape[1:]) == 2:\n",
    "        #return [\"\".join([\"{}{}\".format(count_iter_items(g), k) for k,g in groupby(cs)])]\n",
    "        return [\"\".join(cs)]\n",
    "    if ops[0,-1,-1]: # insertion\n",
    "        traces += ( backtrace_css_gapped(ops[:,:,:-1], s1len, ['I'] + cs) )\n",
    "    if ops[1,-1,-1]: # deletion\n",
    "        traces += ( backtrace_css_gapped(ops[:,:-1,:], s1len, ['D'] + cs) )\n",
    "    if ops[2,-1,-1]: # match\n",
    "        traces += ( backtrace_css_gapped(ops[:,:-1,:-1], s1len, ['='] + cs) )\n",
    "    if ops[3,-1,-1]: # mismatch\n",
    "        traces += ( backtrace_css_gapped(ops[:,:-1,:-1], s1len, ['X'] + cs) )\n",
    "    if ops[4,-1,-1]: # free gap\n",
    "        if ops.shape[1] > s1len + 1:\n",
    "            jump_to = s1len + 1\n",
    "            char = '>'\n",
    "        else:\n",
    "            jump_to = ops[5,:,-1].argmax() + 1\n",
    "            char = '<'\n",
    "        stepsize = ops.shape[1] - jump_to\n",
    "        traces += ( backtrace_css_gapped(ops[:,:jump_to,:], s1len, [char]*stepsize + cs) )\n",
    "    return traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gapped_twinalign(query, subj1, subj2, m=1, mm=-2, go=-4, ge=-1):\n",
    "    qlen, s1len, s2len = len(query), len(subj1), len(subj2)\n",
    "    subj = subj1 + subj2\n",
    "    min_val = np.iinfo(np.int32).min\n",
    "    # stores operations (insertion, deletion, match, mismatch, endgap_start, endgap_end) that maximize the score\n",
    "    ops = np.zeros(shape=(6, s1len+s2len+1, qlen+1), dtype=np.bool_)\n",
    "    ops[2,0,0] = True\n",
    "    ops[0,0,1:] = True\n",
    "    ops[1,1:s1len,0] = True\n",
    "    ops[4,s1len:,0] = True\n",
    "    ops[5,0,0] = True\n",
    "    ops[5,s1len,0] = True\n",
    "    # stores the best partial alignment scores\n",
    "    scores = np.empty(shape=(s1len+s2len+1, qlen+1), dtype=np.int32)\n",
    "    scores[0,0] = 0\n",
    "    scores[0,1:] = np.arange(go + ge, go + (qlen+1)*ge, ge)\n",
    "    scores[1:s1len+1,0] = np.arange(go + ge, go + (s1len+1)*ge, ge)\n",
    "    \n",
    "    op_scores = np.empty(shape=(5,), dtype=np.int32)\n",
    "    for i in range(1,s1len):\n",
    "        for j in range(1,qlen+1):\n",
    "            # insertion\n",
    "            op_scores[0] = scores[i,j-1] + (not ops[0,i,j-1]) * go + ge\n",
    "            # deletion\n",
    "            op_scores[1] = scores[i-1,j] + (not ops[1,i-1,j]) * go + ge\n",
    "            # match/mismatch\n",
    "            if (subj[i-1] == query[j-1]):\n",
    "                op_scores[2] = scores[i-1,j-1] + m\n",
    "                op_scores[3] = min_val\n",
    "            else:\n",
    "                op_scores[2] = min_val\n",
    "                op_scores[3] = scores[i-1,j-1] + mm\n",
    "            \n",
    "            scores[i,j] = op_scores[:4].max()\n",
    "            ops[:4,i,j] = op_scores[:4] == scores[i,j]\n",
    "    for i in range(s1len,s1len+1):\n",
    "        for j in range(1,qlen+1):\n",
    "            # insertion\n",
    "            op_scores[0] = scores[i,j-1] + (not ops[0,i,j-1]) * go + ge\n",
    "            # deletion\n",
    "            op_scores[1] = scores[i-1,j] + (not ops[1,i-1,j]) * go + ge\n",
    "            # match/mismatch\n",
    "            if (subj[i-1] == query[j-1]):\n",
    "                op_scores[2] = scores[i-1,j-1] + m\n",
    "                op_scores[3] = min_val\n",
    "            else:\n",
    "                op_scores[2] = min_val\n",
    "                op_scores[3] = scores[i-1,j-1] + mm\n",
    "            # free end gap\n",
    "            max_in_col = scores[:i,j].argmax()\n",
    "            op_scores[4] = scores[max_in_col,j]\n",
    "            \n",
    "            scores[i,j] = op_scores.max()\n",
    "            ops[:4,i,j] = op_scores[:4] == scores[i,j]\n",
    "            if op_scores[4] == scores[i,j]:\n",
    "                ops[4,i,j] = True # gap start\n",
    "                ops[5,max_in_col,j] = True # gap end\n",
    "            #else:\n",
    "            #    ops[5,i,j] = True # gap end\n",
    "            ops[5,i,j] = True # gap end\n",
    "    scores[s1len:,0] = 0\n",
    "    for i in range(s1len+1,s1len+s2len+1):\n",
    "        for j in range(1,qlen+1):\n",
    "            # insertion\n",
    "            op_scores[0] = scores[i,j-1] + (not ops[0,i,j-1]) * go + ge\n",
    "            # deletion\n",
    "            op_scores[1] = scores[i-1,j] + (not ops[1,i-1,j]) * go + ge\n",
    "            # match/mismatch\n",
    "            if (subj[i-1] == query[j-1]):\n",
    "                op_scores[2] = scores[i-1,j-1] + m\n",
    "                op_scores[3] = min_val\n",
    "            else:\n",
    "                op_scores[2] = min_val\n",
    "                op_scores[3] = scores[i-1,j-1] + mm\n",
    "            # free start gap\n",
    "            op_scores[4] = scores[s1len,j]\n",
    "            \n",
    "            scores[i,j] = op_scores.max()\n",
    "            ops[:5,i,j] = op_scores == scores[i,j]\n",
    "            if ops[4,i,j]:\n",
    "                ops[5,s1len,j] = True # gap end\n",
    "    return scores, ops#np.packbits(ops, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_row(row):\n",
    "    scores,_ = gapped_twinalign(row.query_seq, row.ref1, row.ref2)\n",
    "    #row['score'] = score\n",
    "    return scores[-1,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_align_row(row):\n",
    "    if pd.isnull(row.ref1):\n",
    "        #row['score'] = np.nan\n",
    "        #row['transisions'] = None\n",
    "        return row\n",
    "    qlen, s1len, s2len = len(row.query_seq), len(row.ref1), len(row.ref2)\n",
    "    query, subj = row.query_seq.encode(\"utf8\"), (row.ref1+row.ref2).encode(\"utf8\")\n",
    "    align(query, subj, \n",
    "          qlen, s1len, s2len,\n",
    "          args.match, args.mismatch, args.gap_open, args.gap_extension,\n",
    "          scores_pp, *ops_pp)\n",
    "    score = scores[s1len+s2len, qlen]\n",
    "    #print(row.query_seq, row.ref1, row.ref2)\n",
    "    #return scores[s1len+s2len, qlen]\n",
    "    \n",
    "    transitions = np.zeros(shape=(s1len+1, s2len+1), dtype=np.bool_)\n",
    "    if transitions.flags['C_CONTIGUOUS'] == False:\n",
    "        transitions = np.ascontiguousarray(transitions, dtype=transitions.dtype)\n",
    "    transitions_pp = (transitions.ctypes.data + np.arange(transitions.shape[0]) * transitions.strides[0]).astype(np.uintp)\n",
    "    backtrace(*ops_pp,\n",
    "              qlen, s1len, s2len,\n",
    "              s1len+s2len, qlen, 0, s2len, \n",
    "              transitions_pp)\n",
    "    transitions_hr = list(zip(*np.where(transitions)))\n",
    "    \n",
    "    row['score'] = score\n",
    "    row['transitions'] = transitions_hr\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_align(query_seq, ref1, ref2):\n",
    "    qlen, s1len, s2len = len(query_seq), len(ref1), len(ref2)\n",
    "    query, subj = query_seq.encode(\"utf8\"), (ref1+ref2).encode(\"utf8\")\n",
    "    align(query, subj, \n",
    "          qlen, s1len, s2len,\n",
    "          args.match, args.mismatch, args.gap_open, args.gap_extension,\n",
    "          scores_pp, *ops_pp)\n",
    "    score = scores[s1len+s2len, qlen]\n",
    "    \n",
    "    transitions = np.zeros(shape=(s1len+1, s2len+1), dtype=np.bool_)\n",
    "    if transitions.flags['C_CONTIGUOUS'] == False:\n",
    "        transitions = np.ascontiguousarray(transitions, dtype=transitions.dtype)\n",
    "    transitions_pp = (transitions.ctypes.data + np.arange(transitions.shape[0]) * transitions.strides[0]).astype(np.uintp)\n",
    "    backtrace(*ops_pp,\n",
    "              qlen, s1len, s2len,\n",
    "              s1len+s2len, qlen, 0, s2len, \n",
    "              transitions_pp)\n",
    "    transitions_hr = list(zip(*np.where(transitions)))\n",
    "\n",
    "    print(score)\n",
    "    print(scores[:s1len+s2len+1, :qlen+1])\n",
    "    print(ops[:,:s1len+s2len+1, :qlen+1])\n",
    "    print(transitions_hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg_pat = re.compile(\"(\\d+)([=XID])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_args(['/vol/seqpro2019/MLinder_SMART/Run00376/barcode16.fastq', '/vol/seqpro2019/MLinder_SMART/analysis/Run00376/MIT52.fa', '/vol/seqpro2019/MLinder_SMART/analysis/Run00376/pUC19_ISCg1.fa', '--prefix', 'barcode01', '--wordsize', '12', '--plot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fq_files = []\n",
    "for entry in args.reads:\n",
    "    if os.path.isfile(entry) and (entry.endswith(\".fastq\") or entry.endswith(\".fq\")):\n",
    "        fq_files.append(entry)\n",
    "    else:\n",
    "        fq_files.extend([os.path.join(entry, f) for f in os.listdir(entry) if os.path.isfile(os.path.join(entry, f)) \\\n",
    "            and (f.endswith(\".fastq\") or f.endswith(\".fq\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter = {}\n",
    "print(\" - reading adapter sequence ...\")\n",
    "for record in SeqIO.parse(args.adapter, \"fasta\"):\n",
    "    adapter[str(record.id)] = str(record.seq)\n",
    "adapter = pd.DataFrame.from_dict(adapter, orient='index', columns=['seq'], dtype='string')\n",
    "print('{:>11} adapter sequence(s) in fasta file'.format(len(adapter)))\n",
    "\n",
    "print(\" - reading genome fasta ...\")\n",
    "genome = {}\n",
    "for record in SeqIO.parse(args.genome, \"fasta\"):\n",
    "    genome[str(record.id)] = str(record.seq)\n",
    "genome = pd.DataFrame.from_dict(genome, orient='index', columns=['seq'], dtype='string')\n",
    "print('{:>11} genomic sequence(s) in fasta file'.format(len(genome)))\n",
    "\n",
    "print(\" - reading fastq files ...\")\n",
    "reads = {}\n",
    "#reads = pd.DataFrame([], {'seq':Seq})\n",
    "for fqFile in fq_files:\n",
    "    for record in SeqIO.parse(fqFile, \"fastq\"):\n",
    "        reads[str(record.id)] = str(record.seq)\n",
    "reads = pd.DataFrame.from_dict(reads, orient='index', columns=['seq'], dtype='string')\n",
    "print(\"{:>11} reads in dataset\\n\".format(len(reads)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dir = '{}_tmp'.format(args.prefix)\n",
    "if os.path.exists(tmp_dir):\n",
    "    shutil.rmtree(tmp_dir)\n",
    "os.makedirs(tmp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" - performing reads to adapter reference mapping ...\")\n",
    "fq_fn = \" \".join(fq_files)\n",
    "ref_fn = args.adapter\n",
    "paf_fn = os.path.join(tmp_dir, \"adapter_alignment.paf\")\n",
    "exit_code = run_minimap2(ref_fn, fq_fn, paf_fn)\n",
    "if exit_code:\n",
    "    print('ERROR: adapter reference mapping failed with exit code', exit_code)\n",
    "    exit(1)\n",
    "ad_algn_df = parse_paf(paf_fn, cigar=True)#.set_index('qid')\n",
    "print(\"{:>11} {:>7} primary alignments against adapter sequence(s)\".format(len(ad_algn_df), \"\"))\n",
    "c = sum(ad_algn_df.strand == '+')\n",
    "print(\"{:>11} {:>5.1f} % against (+) strand\".format(c, c/len(ad_algn_df)*100.))\n",
    "c = sum(ad_algn_df.strand == '-')\n",
    "print(\"{:>11} {:>5.1f} % against (-) strand\".format(c, c/len(ad_algn_df)*100.))\n",
    "c = len(set(ad_algn_df.qid))\n",
    "print(\"{:>11} {:>5.1f} % of reads align against any adapter sequence\".format(c, c/len(reads)*100.))\n",
    "#df = pd.merge(df, algn_df, how='outer', left_index=True, right_index=True, sort=False, suffixes=('_x', '_y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" - performing reads to genome reference mapping ...\")\n",
    "ref_fn = args.genome\n",
    "paf_fn = os.path.join(tmp_dir, \"genome_alignment.paf\")\n",
    "exit_code = run_minimap2(ref_fn, fq_fn, paf_fn)\n",
    "if exit_code:\n",
    "    print('ERROR: adapter reference mapping failed with exit code', exit_code)\n",
    "    exit(1)\n",
    "gn_algn_df = parse_paf(paf_fn, cigar=True)#.set_index('qid')\n",
    "print(\"{:>11} {:>7} primary alignments against genomic sequence(s)\".format(len(gn_algn_df), \"\"))\n",
    "c = sum(gn_algn_df.strand == '+')\n",
    "print(\"{:>11} {:>5.1f} % against (+) strand\".format(c, c/len(gn_algn_df)*100.))\n",
    "c = sum(gn_algn_df.strand == '-')\n",
    "print(\"{:>11} {:>5.1f} % against (-) strand\".format(c, c/len(gn_algn_df)*100.))\n",
    "c = len(set(gn_algn_df.qid))\n",
    "print(\"{:>11} {:>5.1f} % of reads align against any genome sequence\".format(c, c/len(reads)*100.))\n",
    "#df = pd.merge(df, algn_df, how='outer', left_index=True, right_index=True, sort=False, suffixes=('_ad', '_gn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (args.mean and args.std):\n",
    "    print(\" - determining statistics about per-base difference ([align. subject len] - [align. query len]) / [align. query len] al from genome alignments\")\n",
    "    sequence_length_stats(gn_algn_df)\n",
    "    print(\"{:>11.4f} mean of per-base difference in sequence length\".format(args.mean))\n",
    "    print(\"{:>11.4f} std dev of per-base difference in sequence length\".format(args.std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" - joining alignment data and filtering for adjacent adapter-genome alignments\")\n",
    "# determine order of alignments of each read with respect to each adapter-subject pair\n",
    "d = pd.concat([ad_algn_df, gn_algn_df], keys=['ad', 'gn'])\n",
    "for ad_id in adapter.index:\n",
    "    for gn_id in genome.index:\n",
    "        comb = [ad_id, gn_id]\n",
    "        o = d.loc[(d.subj.isin(comb))].sort_values(by=[\"qid\", \"qst\"], ascending=True)\n",
    "        d.loc[o.index, \":\".join(comb)] = np.arange(len(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ad = pd.merge(pd.DataFrame(reads.index, columns=['rid']).set_index('rid', drop=False), d.loc[d.index.droplevel(1) == 'ad'].set_index('qid', drop=False),\n",
    "                how='outer', left_index=True, right_index=True, sort=False)\n",
    "#d_gn = pd.merge(df, d.loc[d.index.droplevel(1) == 'gn'].set_index('qid', drop=False),\n",
    "#                how='outer', left_index=True, right_index=True, sort=False)\n",
    "df = pd.merge(d_ad, \n",
    "              d.loc[d.index.droplevel(1) == 'gn'].set_index('qid', drop=False), \n",
    "              how='outer', left_index=True, right_index=True, sort=False, suffixes=('_ad', '_gn'))\n",
    "# reset index\n",
    "df = df.reset_index(drop=True)\n",
    "sel = df.subj_ad.notnull() & \\\n",
    "      df.subj_gn.notnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine if the adapter alignment and the genome alignment are adjacent to each other with respect to \n",
    "# all other alignments of the given read to the same adapter and genome sequence\n",
    "for ad_id in adapter.index:\n",
    "    for gn_id in genome.index:\n",
    "        sel_ = (df.subj_ad == ad_id) & (df.subj_gn == gn_id)\n",
    "        df.loc[sel_, 'align_dist'] = (df[sel_][\"{}:{}_ad\".format(ad_id, gn_id)] - df[sel_][\"{}:{}_gn\".format(ad_id, gn_id)]).abs()\n",
    "print('{:>11} {:>7} entities after joining'.format(len(df), \"\"))\n",
    "c = sum(np.logical_not(sel))\n",
    "print('{:>11} {:>5.1f} % not aligning against both an adapter and a genomic seq'.format(c, c/len(df)*100.))\n",
    "c = sum((df.align_dist > 1.))\n",
    "print('{:>11} {:>5.1f} % are entries are alignments that are not adjacent to each other'.format(c, c/len(df)*100.))\n",
    "sel &= (df.align_dist == 1.)\n",
    "print()\n",
    "c = len(set(df.loc[sel, 'rid']))\n",
    "print('{:>11} {:>5.1f} % reads remaining that contain potential transitions between adapter and genomic seq.'.format(c, c/len(reads)*100.))\n",
    "c = sum(sel)\n",
    "print('{:>11} {:>5.1f} % potential transitions remaining'.format(c, c/len(df)*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting non-potential transitions at this point so that all procentual values are with respect to potential ones only\n",
    "print(\" - deleting all entries from dataframe that are not potential transitions\")\n",
    "df = df.drop(df.index[np.logical_not(sel)]).astype({\"qlen_ad\":np.int32, \"qst_ad\":np.int32, \"qen_ad\":np.int32, \"slen_ad\":np.int32,\n",
    "                                                   \"sst_ad\":np.int32, \"sen_ad\":np.int32, \"mlen_ad\":np.int32, \"blen_ad\":np.int32, \"mapq_ad\":np.int32,\n",
    "                                                   \"qlen_gn\":np.int32, \"qst_gn\":np.int32, \"qen_gn\":np.int32, \"slen_gn\":np.int32,\n",
    "                                                   \"sst_gn\":np.int32, \"sen_gn\":np.int32, \"mlen_gn\":np.int32, \"blen_gn\":np.int32, \"mapq_gn\":np.int32})\n",
    "sel = df.subj_ad.notnull() & \\\n",
    "      df.subj_gn.notnull() & \\\n",
    "      (df.align_dist == 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify if row describes a transition from adapter to genomic sequence or vise versa\n",
    "df.loc[sel,'trans_order'] = 0 # qst_ad == qst_gn\n",
    "df.loc[sel & (df.qst_ad < df.qst_gn), 'trans_order'] = 1 # adapter -> genome\n",
    "df.loc[sel & (df.qst_ad > df.qst_gn), 'trans_order'] = -1 # genome -> adapter\n",
    "c = sum(df[sel].qst_ad < df[sel].qst_gn)\n",
    "print('{:>11} {:>5.1f} % of total potential transpositions are adapter -> genome'.format(c, c/len(df[sel])*100.))\n",
    "c = sum(df[sel].qst_ad > df[sel].qst_gn)\n",
    "print('{:>11} {:>5.1f} % of total potential transpositions are genome -> adapter'.format(c, c/len(df[sel])*100.))\n",
    "\n",
    "# select all entries with an sufficiently long alignment to both the adapter and the genome\n",
    "print(' - filter potential subject transitions based on alignment lengths')\n",
    "sel_ = (df.blen_ad >= args.min_adapter_blen) & \\\n",
    "       (df.blen_gn >= args.min_genome_blen)\n",
    "c = sum(sel & np.logical_not(sel_))\n",
    "print('{:>11} {:>5.1f} % of total potential transpositions filtered'.format(c, c/len(df)*100.))\n",
    "c = sum(sel & np.logical_not(df.blen_ad >= args.min_adapter_blen))\n",
    "print('{:>11} {:>5.1f} % bc adapter alignment length < {}'.format(c, c/len(df)*100., args.min_adapter_blen))\n",
    "c = sum(sel & np.logical_not(df.blen_gn >= args.min_genome_blen))\n",
    "print('{:>11} {:>5.1f} % bc genome alignment length < {}'.format(c, c/len(df)*100., args.min_genome_blen))\n",
    "sel &= sel_\n",
    "c = sum(sel)\n",
    "print('{:>11} {:>5.1f} % potential transitions remaining'.format(c, c/len(df)*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine query seq start and end in read coordinates\n",
    "print(\" - determine query seq start and end in read coordinates\")\n",
    "df = get_query_seq(df, sel)\n",
    "c = sum(sel & (df.query_st.isnull() | df.query_en.isnull()))\n",
    "print('{:>11} {:>5.1f} % of remaining potential transpositions had < {} matching terminal bases'.format(c, c/sum(sel)*100., args.wordsize))\n",
    "sel &= df.query_st.notnull() & df.query_en.notnull()\n",
    "c = sum(sel)\n",
    "print('{:>11} {:>5.1f} % potential transitions remaining'.format(c, c/len(df)*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out entries with query seq. that are too long -> distance between adapter and genome alignment is too large\n",
    "print(\" - exclude entries based on query sequence length (adapter-genome alignment distance) from analysis\")\n",
    "too_short = (df.query_en - df.query_st) < 0.\n",
    "too_long = (df.query_en - df.query_st) > args.max_dist\n",
    "c = sum(too_short)\n",
    "print('{:>11} {:>5.1f} % of remaining potential transpositions removed due to query seq. length < 0 nt (alignment overlap of more than {} nt)'.format(c, c/sum(sel)*100., args.strip))\n",
    "c = sum(too_long)\n",
    "print('{:>11} {:>5.1f} % of remaining potential transpositions removed due to query seq. length > {} nt'.format(c, c/sum(sel)*100., args.max_dist))\n",
    "sel &= np.logical_not(too_short) & np.logical_not(too_long)\n",
    "c = sum(sel)\n",
    "print('{:>11} {:>5.1f} % potential transitions remaining'.format(c, c/len(df)*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set query seq for each row\n",
    "print(\" - determining query sequences\")\n",
    "df.loc[sel, 'query_seq'] = df[sel].apply(lambda row: reads.loc[row.rid].seq[int(row.query_st) : int(row.query_en)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine reference sequences\n",
    "print(\" - determining reference sequences\")\n",
    "#df.loc[sel, 'min_ref_len'] = ((df[sel].query_en - df[sel].query_st) * (1 + args.mean - 3 * args.std) - 0.5).round()\n",
    "df.loc[sel, 'max_ref_len'] = ((df[sel].query_en - df[sel].query_st) * (1 + args.mean + 3 * args.std) + 0.5).round()\n",
    "\n",
    "# set first and second reference seq\n",
    "sel_ = sel & (df.trans_order == 1.) & (df.strand_ad == '+')\n",
    "df.loc[sel_, 'ref1'] = df[sel_].apply(\n",
    "    lambda row: adapter.loc[row.subj_ad].seq[(row.sen_ad - row.ref1_trim) : (row.sen_ad - row.ref1_trim) + int(row.max_ref_len)], axis=1)\n",
    "sel_ = sel & (df.trans_order == 1.) & (df.strand_ad == '-')\n",
    "df.loc[sel_, 'ref1'] = df[sel_].apply(\n",
    "    lambda row: adapter.loc[row.subj_ad].seq[(row.sst_ad + row.ref1_trim) - int(row.max_ref_len) : (row.sst_ad + row.ref1_trim)], axis=1)\n",
    "df.loc[sel_, 'ref1'] = df.loc[sel_, 'ref1'].str.translate(compl).str[::-1] # reverse complement\n",
    "\n",
    "\n",
    "sel_ = sel & (df.trans_order == 1.) & (df.strand_gn == '+')\n",
    "df.loc[sel_, 'ref2'] = df[sel_].apply(\n",
    "    lambda row: genome.loc[row.subj_gn].seq[(row.sst_gn + row.ref2_trim) - int(row.max_ref_len) : (row.sst_gn + row.ref2_trim)], axis=1)\n",
    "sel_ = sel & (df.trans_order == 1.) & (df.strand_gn == '-')\n",
    "df.loc[sel_, 'ref2'] = df[sel_].apply(\n",
    "    lambda row: genome.loc[row.subj_gn].seq[(row.sen_gn - row.ref2_trim) : (row.sen_gn - row.ref2_trim) + int(row.max_ref_len)], axis=1)\n",
    "df.loc[sel_, 'ref2'] = df.loc[sel_, 'ref2'].str.translate(compl).str[::-1] # reverse complement\n",
    "\n",
    "\n",
    "sel_ = sel & (df.trans_order == -1.) & (df.strand_gn == '+')\n",
    "df.loc[sel_, 'ref1'] = df[sel_].apply(\n",
    "    lambda row: genome.loc[row.subj_gn].seq[(row.sen_gn - row.ref1_trim) : (row.sen_gn - row.ref1_trim) + int(row.max_ref_len)], axis=1)\n",
    "sel_ = sel & (df.trans_order == -1.) & (df.strand_gn == '-')\n",
    "df.loc[sel_, 'ref1'] = df[sel_].apply(\n",
    "    lambda row: genome.loc[row.subj_gn].seq[(row.sst_gn + row.ref1_trim) - int(row.max_ref_len) : (row.sst_gn + row.ref1_trim)], axis=1)\n",
    "df.loc[sel_, 'ref1'] = df.loc[sel_, 'ref1'].str.translate(compl).str[::-1] # reverse complement\n",
    "\n",
    "\n",
    "sel_ = sel & (df.trans_order == -1.) & (df.strand_ad == '+')\n",
    "df.loc[sel_, 'ref2'] = df[sel_].apply(\n",
    "    lambda row: adapter.loc[row.subj_ad].seq[(row.sst_ad + row.ref2_trim) - int(row.max_ref_len) : (row.sst_ad + row.ref2_trim)], axis=1)\n",
    "sel_ = sel & (df.trans_order == -1.) & (df.strand_ad == '-')\n",
    "df.loc[sel_, 'ref2'] = df[sel_].apply(\n",
    "    lambda row: adapter.loc[row.subj_ad].seq[(row.sen_ad - row.ref2_trim) : (row.sen_ad - row.ref2_trim) + int(row.max_ref_len)], axis=1)\n",
    "df.loc[sel_, 'ref2'] = df.loc[sel_, 'ref2'].str.translate(compl).str[::-1] # reverse complement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' - aligning')\n",
    "# initialize the necessary data structures\n",
    "wd = !pwd\n",
    "print(os.path.join(str(wd[0]), \"align.so\"))\n",
    "clib = os.path.join(str(wd[0]), \"align.so\")\n",
    "\n",
    "nd_pp = np.ctypeslib.ndpointer(dtype=np.uintp, ndim=1, flags='C_CONTIGUOUS')\n",
    "align = ct.CDLL(clib).align\n",
    "align.argtypes = [ct.c_char_p, ct.c_char_p, \n",
    "                  ct.c_short, ct.c_short, ct.c_short,\n",
    "                  ct.c_short, ct.c_short, ct.c_short, ct.c_short,\n",
    "                  nd_pp, nd_pp, nd_pp, nd_pp, nd_pp, nd_pp, nd_pp]\n",
    "align.restype = ct.c_int\n",
    "\n",
    "backtrace = ct.CDLL(clib).backtrace2\n",
    "backtrace.argtypes = [nd_pp, nd_pp, nd_pp, nd_pp, nd_pp, nd_pp,\n",
    "                      ct.c_short, ct.c_short, ct.c_short,\n",
    "                      nd_pp]\n",
    "backtrace.restype = ct.c_int\n",
    "\n",
    "ct.CDLL(clib).init()\n",
    "\n",
    "max_subj_len = 2*int((args.max_dist * (1 + args.mean + 3 * args.std) + 0.5).round())\n",
    "print(max_subj_len)\n",
    "scores = np.empty(shape=(max_subj_len+1, args.max_dist+1), dtype=np.int16)\n",
    "ops = np.empty(shape=(6,max_subj_len+1, args.max_dist+1), dtype=np.bool_)\n",
    "#m, mm, go, ge = 1, -2, -4, -1\n",
    "# initialize the fields that never change\n",
    "scores[0,0] = 0\n",
    "scores[0,1:] = np.arange(args.gap_open + args.gap_extension, args.gap_open + (args.max_dist+1)*args.gap_extension, args.gap_extension)\n",
    "ops[:,0,:] = False\n",
    "ops[:,:,0] = False\n",
    "ops[0,0,1:] = True\n",
    "ops[2,0,0] = True\n",
    "ops[5,0,0] = True\n",
    "if scores.flags['C_CONTIGUOUS'] == False:\n",
    "    scores = np.ascontiguousarray(scores, dtype=scores.dtype)\n",
    "if ops.flags['C_CONTIGUOUS'] == False:\n",
    "    ops = np.ascontiguousarray(ops, dtype=ops.dtype)\n",
    "scores_pp = (scores.ctypes.data + np.arange(scores.shape[0]) * scores.strides[0]).astype(np.uintp)\n",
    "ops_pp = [(ops[i].ctypes.data + np.arange(ops[i].shape[0]) * ops[i].strides[0]).astype(np.uintp) for i in range(6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandarallel.initialize(nb_workers=1, progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "8\n",
      "14\n",
      "15\n",
      "25\n",
      "26\n",
      "28\n",
      "42\n",
      "44\n",
      "48\n",
      "55\n",
      "62\n",
      "68\n",
      "69\n",
      "82\n",
      "86\n",
      "93\n",
      "98\n",
      "102\n",
      "125\n",
      "126\n",
      "132\n",
      "133\n",
      "143\n",
      "145\n",
      "152\n",
      "166\n",
      "167\n",
      "173\n",
      "179\n",
      "185\n",
      "187\n",
      "190\n",
      "206\n",
      "215\n",
      "222\n",
      "227\n",
      "237\n",
      "240\n",
      "244\n",
      "252\n",
      "263\n",
      "264\n",
      "279\n",
      "282\n",
      "299\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "308\n",
      "313\n",
      "315\n",
      "354\n"
     ]
    }
   ],
   "source": [
    "for i,row in df[:500].iterrows():\n",
    "    print(i)\n",
    "    c_align_row(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAGCCAGTTAACTGGATCAGCTGTCCCCGTCTGAAACCATTCCCACTCAAGATTAACAATATGTACGCCCTTCTTCGCTTCGGCGGGTAGCTCATCCAACCATTGCGCCGTCTTTTTGTCGACTACCATGCCCACCAACGAGATCGGAAGAGCACACGTCTGCGTGC GAGCCGCAAAATACTGCTCAGACGCGTTAGAGTGCATTGATCTTATGGACCAACTGCCCTGAATGGATAAGGCACCGCAGAATGTAGTGGTTCAAATTACGGAAACCTAGAGCAATCCCACGCAAATGCTCCAACCGTCCGTTGATCGCTTCGACCGGACCGTTGGAGACACCAACATCGAAA GCGGCCAGTCGATCATGTTGAGGTTGACGTTGGCGTAATGCGCGTCGGGGGACCAATGATCGTTGACGAGGTAGGTGTACAGCGTGCCGTCCTCGAGCGCCTGGAAGCCGTTTGCCACACCACGCGGGACGTAAACTCCCACGTCAGGGGTAATTTTTTGCGTTACGACGTTACCGTACGTGC\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[354].query_seq, df.loc[354].ref1, df.loc[354].ref2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "c_align_row(df.loc[354])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# start the alignment\n",
    "df[20:500].apply(lambda row: c_align_row(row), axis=1)\n",
    "#df.loc[sel & (df.query_seq.str.len() > 0), 'norm_score'] = df[sel].score / df[sel].query_seq.str.len()\n",
    "#df.loc[sel & (df.query_seq.str.len() == 0), 'norm_score'] = args.match # if the two alignments are fitting together perfetly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[sel & (df.query_seq.str.len() == 0), [\"query_seq\", \"ref1\", \"ref2\", \"score\", \"norm_score\", \"transitions\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_norm_score_distribution(df, nbins=50):\n",
    "    x, y = df.query_seq.str.len(), df.norm_score\n",
    "    \n",
    "    # definitions for the axes\n",
    "    left, width = 0.1, 0.65\n",
    "    bottom, height = 0.1, 0.65\n",
    "    spacing = 0.005\n",
    "\n",
    "    rect_scatter = [left, bottom, width, height]\n",
    "    rect_histx = [left, bottom + height + spacing, width, 0.2]\n",
    "    rect_histy = [left + width + spacing, bottom, 0.2, height]\n",
    "\n",
    "    # start with a rectangular Figure\n",
    "    plt.figure(figsize=(8, 8))\n",
    "\n",
    "    ax_scatter = plt.axes(rect_scatter)\n",
    "    ax_scatter.tick_params(direction='in', top=True, right=True)\n",
    "    ax_histx = plt.axes(rect_histx)\n",
    "    ax_histx.tick_params(direction='in', labelbottom=False)\n",
    "    ax_histy = plt.axes(rect_histy)\n",
    "    ax_histy.tick_params(direction='in', labelleft=False)\n",
    "\n",
    "    ax_scatter.scatter(x, y, s=1., alpha=0.05)\n",
    "\n",
    "    ax_histx.hist(x, bins=nbins)\n",
    "    ax_histy.hist(y, bins=nbins, orientation='horizontal')\n",
    "\n",
    "    ax_histx.set_xlim(ax_scatter.get_xlim())\n",
    "    ax_histy.set_ylim(ax_scatter.get_ylim())\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.plot:\n",
    "    plot_norm_score_distribution(df[sel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d = df.loc[sel & (df.norm_score >= 0.5), ['trans_order','strand_ad','subj_ad','sst_ad', 'sen_ad', 'subj_gn','sst_gn', 'sen_gn','strand_gn','max_ref_len','norm_score','transitions']].explode('transitions')\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set first and second reference seq\n",
    "sel_ = (d.trans_order == 1.) & (d.strand_ad == '+')\n",
    "d.loc[sel_, 'ts'] = d[sel_].apply(\n",
    "    lambda row: (row.sen_ad - args.strip) + row.transitions[0], axis=1)\n",
    "    #lambda row: adapter.loc[row.subj_ad].seq[(row.sen_ad - args.strip) : (row.sen_ad - args.strip) + int(row.max_ref_len)], axis=1)\n",
    "sel_ = (d.trans_order == 1.) & (d.strand_ad == '-')\n",
    "d.loc[sel_, 'ts'] = d[sel_].apply(\n",
    "    lambda row: (row.sst_ad + args.strip) - row.transitions[0], axis=1)\n",
    "    #lambda row: adapter.loc[row.subj_ad].seq[(row.sst_ad + args.strip) - int(row.max_ref_len) : (row.sst_ad + args.strip)], axis=1)\n",
    "\n",
    "sel_ = (d.trans_order == 1.) & (d.strand_gn == '+')\n",
    "d.loc[sel_, 'te'] = d[sel_].apply(\n",
    "    lambda row: (row.sst_gn + args.strip) - int(row.max_ref_len) + row.transitions[1], axis=1)\n",
    "    #lambda row: genome.loc[row.subj_gn].seq[(row.sst_gn + args.strip) - int(row.max_ref_len) : (row.sst_gn + args.strip)], axis=1)\n",
    "sel_ = (d.trans_order == 1.) & (d.strand_gn == '-')\n",
    "d.loc[sel_, 'te'] = d[sel_].apply(\n",
    "    lambda row: (row.sen_gn - args.strip) + int(row.max_ref_len) - row.transitions[1], axis=1)\n",
    "    #lambda row: genome.loc[row.subj_gn].seq[(row.sen_gn - args.strip) : (row.sen_gn - args.strip) + int(row.max_ref_len)], axis=1)\n",
    "\n",
    "sel_ = (d.trans_order == -1.) & (d.strand_gn == '+')\n",
    "d.loc[sel_, 'ts'] = d[sel_].apply(\n",
    "    lambda row: (row.sen_gn - args.strip) + row.transitions[0], axis=1)\n",
    "    #lambda row: genome.loc[row.subj_gn].seq[(row.sen_gn - args.strip) : (row.sen_gn - args.strip) + int(row.max_ref_len)], axis=1)\n",
    "sel_ = (d.trans_order == -1.) & (d.strand_gn == '-')\n",
    "d.loc[sel_, 'ts'] = d[sel_].apply(\n",
    "    lambda row: (row.sst_gn + args.strip) - row.transitions[0], axis=1)\n",
    "    #lambda row: genome.loc[row.subj_gn].seq[(row.sst_gn + args.strip) - int(row.max_ref_len) : (row.sst_gn + args.strip)], axis=1)\n",
    "\n",
    "sel_ = (d.trans_order == -1.) & (d.strand_ad == '+')\n",
    "d.loc[sel_, 'te'] = d[sel_].apply(\n",
    "    lambda row: (row.sst_ad + args.strip) - int(row.max_ref_len) + row.transitions[1], axis=1)\n",
    "    #lambda row: adapter.loc[row.subj_ad].seq[(row.sst_ad + args.strip) - int(row.max_ref_len) : (row.sst_ad + args.strip)], axis=1)\n",
    "sel_ = (df.trans_order == -1.) & (df.strand_ad == '-')\n",
    "d.loc[sel_, 'te'] = d[sel_].apply(\n",
    "    lambda row: (row.sen_ad - args.strip) + int(row.max_ref_len) - row.transitions[1], axis=1)\n",
    "    #lambda row: adapter.loc[row.subj_ad].seq[(row.sen_ad - args.strip) : (row.sen_ad - args.strip) + int(row.max_ref_len)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
