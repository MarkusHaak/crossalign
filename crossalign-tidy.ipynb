{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, argparse, shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "from itertools import groupby, count\n",
    "from collections import deque\n",
    "from io import StringIO\n",
    "import ctypes as ct\n",
    "import re\n",
    "from pandarallel import pandarallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate for building complement of a DNA sequence\n",
    "compl = str.maketrans('ATGCNatgcn', 'TACGNatgcn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgHelpFormatter(argparse.HelpFormatter):\n",
    "    '''\n",
    "    Formatter adding default values to help texts.\n",
    "    '''\n",
    "    def __init__(self, prog):\n",
    "        super().__init__(prog)\n",
    "\n",
    "    ## https://stackoverflow.com/questions/3853722\n",
    "    #def _split_lines(self, text, width):\n",
    "    #   if text.startswith('R|'):\n",
    "    #       return text[2:].splitlines()  \n",
    "    #   # this is the RawTextHelpFormatter._split_lines\n",
    "    #   return argparse.HelpFormatter._split_lines(self, text, width)\n",
    "\n",
    "    def _get_help_string(self, action):\n",
    "        text = action.help\n",
    "        if  action.default is not None and \\\n",
    "            action.default != argparse.SUPPRESS and \\\n",
    "            'default:' not in text.lower():\n",
    "            text += ' (default: {})'.format(action.default)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args(args=None):\n",
    "    parser = argparse.ArgumentParser(description='Estimates read starts (transposase insertion sites) for ONT rapid libraries',\n",
    "                                     formatter_class=ArgHelpFormatter, \n",
    "                                     add_help=False)\n",
    "\n",
    "    main_group = parser.add_argument_group('Main Options')\n",
    "    main_group.add_argument('reads',\n",
    "                            nargs='+',\n",
    "                            help='fastq files or path to directories containing fastq files (recursion depth 1)')\n",
    "    main_group.add_argument('genome',\n",
    "                            help='Fasta file containing the genomic sequences that is searched for insertion sites.')\n",
    "    main_group.add_argument('adapter',\n",
    "                            help='Transposon Y adapter sequence')\n",
    "    main_group.add_argument('--prefix',\n",
    "                            help=\"filename for readstarts in tap seperated value (.tsv) format\",\n",
    "                            default=\"readstarts\")\n",
    "    #main_group.add_argument('--verbose_out_file',\n",
    "    #                        help=\"print detailed, human readable information about each read to this file\")\n",
    "    #main_group.add_argument('--verbose_col_width',\n",
    "    #                        help=\"column width for verbose output\",\n",
    "    #                        type=int,\n",
    "    #                        default=250)\n",
    "    main_group.add_argument('--plot',\n",
    "                            help='plot results of gaussian approximation',\n",
    "                            action='store_true')\n",
    "    main_group.add_argument('--circular',\n",
    "                            action=\"store_true\")\n",
    "    main_group.add_argument('--strip',\n",
    "                            help=\"number of bases stripped from alignments to cope with coincidently identical sequences\",\n",
    "                            type=int,\n",
    "                            default=5)\n",
    "    main_group.add_argument('--wordsize',\n",
    "                            help='',\n",
    "                            type=int,\n",
    "                            default=8)\n",
    "    main_group.add_argument('--max_dist',\n",
    "                            help='''max distance between an adapter and a genome alignment to perform pairwise-alignment \n",
    "                                 in order to identify the exact transition point''',\n",
    "                            type=int,\n",
    "                            default=200)\n",
    "    main_group.add_argument('--min_readlength',\n",
    "                            help=\"min length of reads to be analyzed\",\n",
    "                            type=int,\n",
    "                            default=500)\n",
    "    main_group.add_argument('--processes',\n",
    "                            type=int,\n",
    "                            default=6)\n",
    "    main_group.add_argument('--batchsize',\n",
    "                            type=int,\n",
    "                            default=8000)\n",
    "    \n",
    "    align_group = parser.add_argument_group('Alignment Options')\n",
    "    align_group.add_argument('--match',\n",
    "                             help='match score',\n",
    "                             type=int,\n",
    "                             default=1)\n",
    "    align_group.add_argument('--mismatch',\n",
    "                             help='mismatch penalty',\n",
    "                             type=int,\n",
    "                             default=-2)\n",
    "    align_group.add_argument('--gap_open',\n",
    "                             help='gap open penalty',\n",
    "                             type=int,\n",
    "                             default=-4)\n",
    "    align_group.add_argument('--gap_extension',\n",
    "                             help='gap extension penalty',\n",
    "                             type=int,\n",
    "                             default=-1)\n",
    "\n",
    "    filter_group = parser.add_argument_group('Filter Options')\n",
    "    filter_group.add_argument('--mean',\n",
    "                              help='mean of per-base difference of actual sequence length from read length',\n",
    "                              type=float)\n",
    "    filter_group.add_argument('--std',\n",
    "                              help='standard deviation of per-base difference of actual sequence length from read length',\n",
    "                              type=float)\n",
    "    filter_group.add_argument('--min_blen',\n",
    "                              help=\"min produced alignment length (including errors)\",\n",
    "                              type=int,\n",
    "                              default=20)\n",
    "    filter_group.add_argument('--min_adapter_blen',\n",
    "                              help=\"min produced alignment length (including errors)\",\n",
    "                              type=int,\n",
    "                              default=50)\n",
    "    filter_group.add_argument('--min_genome_blen',\n",
    "                              help=\"min produced alignment length (including errors)\",\n",
    "                              type=int,\n",
    "                              default=50)\n",
    "    filter_group.add_argument('--f_window',\n",
    "                              help=\"sequence window around the position of transition from \"+\\\n",
    "                                   \"the adapter sequence to the chromosome sequence\",\n",
    "                              type=int,\n",
    "                              default=7)\n",
    "    filter_group.add_argument('--f_max_w_err',\n",
    "                              help=\"max amount of errors (insertions, deletions, mismatches) \"+\\\n",
    "                                   \"within the specified sequence window\",\n",
    "                              type=int,\n",
    "                              default=3)\n",
    "    filter_group.add_argument('--f_max_mm_strech',\n",
    "                              help=\"max amount of deletions or insertions in the whole realignment\",\n",
    "                              type=int,\n",
    "                              default=3)\n",
    "\n",
    "    help_group = parser.add_argument_group('Help')\n",
    "    help_group.add_argument('-h', '--help', \n",
    "                            action='help', \n",
    "                            default=argparse.SUPPRESS,\n",
    "                            help='Show this help message and exit.')\n",
    "    if args:\n",
    "        return parser.parse_args(args)\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_minimap2(ref_fn, fq_fn, paf_fn):\n",
    "    cmd ='minimap2 -x map-ont -c --eqx --secondary=no -t 4 {} {} >{} 2> /dev/null'.format(ref_fn, fq_fn, paf_fn)\n",
    "    #print('Running:', cmd)\n",
    "    return os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_paf(fn, cigar=False):\n",
    "    usecols = list(range(12))\n",
    "    names = [\"qid\", \"qlen\", \"qst\", \"qen\", \"strand\", \"subj\", \n",
    "             \"slen\", \"sst\", \"sen\", \"mlen\", \"blen\", \"mapq\"]\n",
    "    dtype = {\"qid\": str, \"qlen\": np.int32, \"qst\": np.int32, \n",
    "             \"qen\": np.int32, \"strand\": str, \"subj\": str,\n",
    "             \"slen\": np.int32, \"sst\": np.int32, \"sen\": np.int32, \n",
    "             \"mlen\": np.int32, \"blen\": np.int32, \"mapq\": np.int32}\n",
    "    converters = {}\n",
    "    if cigar:\n",
    "        usecols.append(22)\n",
    "        names.append('cg')\n",
    "        converters['cg'] = lambda x: x.split(':')[-1]\n",
    "    return pd.read_csv(fn, sep='\\t', header=None,\n",
    "                       usecols=usecols,\n",
    "                       names=names,\n",
    "                       dtype=dtype,\n",
    "                       converters=converters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_query_st(row):\n",
    "    trimmed_query = 0\n",
    "    trimmed_subject = 0\n",
    "    if row.trans_order == 1.:\n",
    "        cg = row.cg_ad\n",
    "        qen = row.qen_ad\n",
    "    else:\n",
    "        cg = row.cg_gn\n",
    "        qen = row.qen_gn\n",
    "    cg_list = re.findall(cg_pat, cg)\n",
    "    for bases,op in reversed(cg_list):\n",
    "        bases = int(bases)\n",
    "        if op == \"=\" and bases >= args.wordsize:\n",
    "            return qen - trimmed_query, trimmed_subject\n",
    "            break\n",
    "        if op == \"=\" or op == 'X':\n",
    "            trimmed_query += bases\n",
    "            trimmed_subject += bases\n",
    "        elif op == 'I':\n",
    "            trimmed_query += bases\n",
    "        else: # == D\n",
    "            trimmed_subject += bases\n",
    "    # unsuccessful\n",
    "    return np.nan, np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_query_en(row):\n",
    "    trimmed_query = 0\n",
    "    trimmed_subject = 0\n",
    "    if row.trans_order == 1.:\n",
    "        cg = row.cg_gn\n",
    "        qst = row.qst_gn\n",
    "    else:\n",
    "        cg = row.cg_ad\n",
    "        qst = row.qst_ad\n",
    "    for m in re.finditer(cg_pat, cg):\n",
    "        bases, op = int(m.group(1)), m.group(2)\n",
    "        if op == \"=\" and bases >= args.wordsize:\n",
    "            return qst + trimmed_query, trimmed_subject\n",
    "            break\n",
    "        if op == \"=\" or op == 'X':\n",
    "            trimmed_query += bases\n",
    "            trimmed_subject += bases\n",
    "        elif op == 'I':\n",
    "            trimmed_query += bases\n",
    "        else: # == D\n",
    "            trimmed_subject += bases\n",
    "    # unsuccessful\n",
    "    return np.nan, np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_seq(df, sel, fix=True):\n",
    "    df['ref1_trim'], df['ref2_trim'] = 0, 0\n",
    "    \n",
    "    cg_ad = df.cg_ad.str.replace('D|I', 'X')\n",
    "    cg_gn = df.cg_gn.str.replace('D|I', 'X')\n",
    "    \n",
    "    sel_ = sel & (df.trans_order == 1.) & (cg_ad.str.rstrip('=').str.rsplit('X', n=1).str[-1].astype(np.float32) >= args.wordsize) # & (df.strand_ad == '+')\n",
    "    df.loc[sel_ , 'query_st'] = df[sel_].qen_ad\n",
    "    sel_ = sel & (df.trans_order == 1.) & (cg_gn.str.split('=', n=1).str[0].astype(np.float32) >= args.wordsize) # & (df.strand_ad == '+')\n",
    "    df.loc[sel_ , 'query_en'] = df[sel_].qst_gn\n",
    "    \n",
    "    sel_ = sel & (df.trans_order == -1.) & (cg_gn.str.rstrip('=').str.rsplit('X', n=1).str[-1].astype(np.float32) >= args.wordsize) # & (df.strand_ad == '+') \n",
    "    df.loc[sel_ , 'query_st'] = df[sel_].qen_gn\n",
    "    sel_ = sel & (df.trans_order == -1.) & (cg_ad.str.split('=', n=1).str[0].astype(np.float32) >= args.wordsize) # & (df.strand_ad == '+')\n",
    "    df.loc[sel_ , 'query_en'] = df[sel_].qst_ad\n",
    "    \n",
    "    if fix:\n",
    "        # fix query end & start of reads with insufficient terminal alignments\n",
    "        df.loc[sel & df.query_st.isnull(), ['query_st', 'ref1_trim']] = pd.DataFrame(\n",
    "            df.loc[sel & df.query_st.isnull()].apply(lambda row: fix_query_st(row), axis=1).values.tolist(), \n",
    "            index=df.loc[sel & df.query_st.isnull()].index, columns=['query_st', 'ref1_trim']\n",
    "        )\n",
    "        df.loc[sel & df.query_en.isnull(), ['query_en', 'ref2_trim']] = pd.DataFrame(\n",
    "            df.loc[sel & df.query_en.isnull()].apply(lambda row: fix_query_en(row), axis=1).values.tolist(), \n",
    "            index=df.loc[sel & df.query_en.isnull()].index, columns=['query_en', 'ref2_trim']\n",
    "        )\n",
    "    \n",
    "    df.loc[sel, 'query_st'] -= args.strip\n",
    "    df.loc[sel, 'query_en'] += args.strip\n",
    "    df.loc[sel, 'ref1_trim'] += args.strip\n",
    "    df.loc[sel, 'ref2_trim'] += args.strip\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_length_stats(df):\n",
    "    dfs = []\n",
    "    # add length of read seq that aligns as \"qalen\"\n",
    "    dfs.append((df[df.subj.notnull()].qen - df[df.subj.notnull()].qst).to_frame(name='qalen'))\n",
    "    # add length of genome seq that aligns as \"salen\"\n",
    "    dfs.append((df[df.subj.notnull()].sen - df[df.subj.notnull()].sst).abs().to_frame(name='salen'))\n",
    "    ## add match, mismatch, insertion and deletion counts\n",
    "    #two_groups = '(?P<digit>[0-9]*)(?P<letter>[=XID])'\n",
    "    #d = df[df.subj_gn.notnull()].cg_gn.str.extractall(two_groups)\n",
    "    #d.loc[:, 'digit'] = d.digit.astype(\"float32\")\n",
    "    #for letter in ['=', 'X', 'I', 'D']:\n",
    "    #    dfs.append(d.loc[d.letter == letter].groupby(level=0).agg('sum').rename(columns = {'digit':letter}))\n",
    "    stats = pd.concat(dfs, axis=1).fillna(0.)\n",
    "    data = (stats.salen - stats.qalen)/stats.qalen\n",
    "    args.mean, args.std = norm.fit(data)\n",
    "    if args.plot:\n",
    "        fig, ax = plt.subplots(figsize=(8,6))\n",
    "        ax.hist(data, bins=100, density=True)\n",
    "        xmin, xmax = ax.get_xlim()\n",
    "        x = np.linspace(xmin, xmax, 200)\n",
    "        y = norm.pdf(x, args.mean, args.std)\n",
    "        ax.plot(x, y)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_iter_items(iterable):\n",
    "        counter = count()\n",
    "        deque(zip(iterable, counter), maxlen=0)  # (consume at C speed)\n",
    "        return next(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cigar_str(query, subj):\n",
    "    cs = []\n",
    "    for q,s in zip(query, subj):\n",
    "        if q == s:\n",
    "            cs.append('=')\n",
    "        elif q == '-':\n",
    "            cs.append('D')\n",
    "        elif s == '-':\n",
    "            cs.append('I')\n",
    "        else:\n",
    "            cs.append('X')\n",
    "    return \"\".join(cs)\n",
    "    #return \"\".join([\"{}{}\".format(count_iter_items(g), k) for k,g in groupby(cs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtrace_css_gapped(ops, s1len, cs=[]):\n",
    "    traces = []\n",
    "    if sum(ops.shape[1:]) == 2:\n",
    "        #return [\"\".join([\"{}{}\".format(count_iter_items(g), k) for k,g in groupby(cs)])]\n",
    "        return [\"\".join(cs)]\n",
    "    if ops[0,-1,-1]: # insertion\n",
    "        traces += ( backtrace_css_gapped(ops[:,:,:-1], s1len, ['I'] + cs) )\n",
    "    if ops[1,-1,-1]: # deletion\n",
    "        traces += ( backtrace_css_gapped(ops[:,:-1,:], s1len, ['D'] + cs) )\n",
    "    if ops[2,-1,-1]: # match\n",
    "        traces += ( backtrace_css_gapped(ops[:,:-1,:-1], s1len, ['='] + cs) )\n",
    "    if ops[3,-1,-1]: # mismatch\n",
    "        traces += ( backtrace_css_gapped(ops[:,:-1,:-1], s1len, ['X'] + cs) )\n",
    "    if ops[4,-1,-1]: # free gap\n",
    "        if ops.shape[1] > s1len + 1:\n",
    "            jump_to = s1len + 1\n",
    "            char = '>'\n",
    "        else:\n",
    "            jump_to = ops[5,:,-1].argmax() + 1\n",
    "            char = '<'\n",
    "        stepsize = ops.shape[1] - jump_to\n",
    "        traces += ( backtrace_css_gapped(ops[:,:jump_to,:], s1len, [char]*stepsize + cs) )\n",
    "    return traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gapped_twinalign(query, subj1, subj2, m=1, mm=-2, go=-4, ge=-1):\n",
    "    qlen, s1len, s2len = len(query), len(subj1), len(subj2)\n",
    "    subj = subj1 + subj2\n",
    "    min_val = np.iinfo(np.int32).min\n",
    "    # stores operations (insertion, deletion, match, mismatch, endgap_start, endgap_end) that maximize the score\n",
    "    ops = np.zeros(shape=(6, s1len+s2len+1, qlen+1), dtype=np.bool_)\n",
    "    ops[2,0,0] = True\n",
    "    ops[0,0,1:] = True\n",
    "    ops[1,1:s1len,0] = True\n",
    "    ops[4,s1len:,0] = True\n",
    "    ops[5,0,0] = True\n",
    "    ops[5,s1len,0] = True\n",
    "    # stores the best partial alignment scores\n",
    "    scores = np.empty(shape=(s1len+s2len+1, qlen+1), dtype=np.int32)\n",
    "    scores[0,0] = 0\n",
    "    scores[0,1:] = np.arange(go + ge, go + (qlen+1)*ge, ge)\n",
    "    scores[1:s1len+1,0] = np.arange(go + ge, go + (s1len+1)*ge, ge)\n",
    "    \n",
    "    op_scores = np.empty(shape=(5,), dtype=np.int32)\n",
    "    for i in range(1,s1len):\n",
    "        for j in range(1,qlen+1):\n",
    "            # insertion\n",
    "            op_scores[0] = scores[i,j-1] + (not ops[0,i,j-1]) * go + ge\n",
    "            # deletion\n",
    "            op_scores[1] = scores[i-1,j] + (not ops[1,i-1,j]) * go + ge\n",
    "            # match/mismatch\n",
    "            if (subj[i-1] == query[j-1]):\n",
    "                op_scores[2] = scores[i-1,j-1] + m\n",
    "                op_scores[3] = min_val\n",
    "            else:\n",
    "                op_scores[2] = min_val\n",
    "                op_scores[3] = scores[i-1,j-1] + mm\n",
    "            \n",
    "            scores[i,j] = op_scores[:4].max()\n",
    "            ops[:4,i,j] = op_scores[:4] == scores[i,j]\n",
    "    for i in range(s1len,s1len+1):\n",
    "        for j in range(1,qlen+1):\n",
    "            # insertion\n",
    "            op_scores[0] = scores[i,j-1] + (not ops[0,i,j-1]) * go + ge\n",
    "            # deletion\n",
    "            op_scores[1] = scores[i-1,j] + (not ops[1,i-1,j]) * go + ge\n",
    "            # match/mismatch\n",
    "            if (subj[i-1] == query[j-1]):\n",
    "                op_scores[2] = scores[i-1,j-1] + m\n",
    "                op_scores[3] = min_val\n",
    "            else:\n",
    "                op_scores[2] = min_val\n",
    "                op_scores[3] = scores[i-1,j-1] + mm\n",
    "            # free end gap\n",
    "            max_in_col = scores[:i,j].argmax()\n",
    "            op_scores[4] = scores[max_in_col,j]\n",
    "            \n",
    "            scores[i,j] = op_scores.max()\n",
    "            ops[:4,i,j] = op_scores[:4] == scores[i,j]\n",
    "            if op_scores[4] == scores[i,j]:\n",
    "                ops[4,i,j] = True # gap start\n",
    "                ops[5,max_in_col,j] = True # gap end\n",
    "            #else:\n",
    "            #    ops[5,i,j] = True # gap end\n",
    "            ops[5,i,j] = True # gap end\n",
    "    scores[s1len:,0] = 0\n",
    "    for i in range(s1len+1,s1len+s2len+1):\n",
    "        for j in range(1,qlen+1):\n",
    "            # insertion\n",
    "            op_scores[0] = scores[i,j-1] + (not ops[0,i,j-1]) * go + ge\n",
    "            # deletion\n",
    "            op_scores[1] = scores[i-1,j] + (not ops[1,i-1,j]) * go + ge\n",
    "            # match/mismatch\n",
    "            if (subj[i-1] == query[j-1]):\n",
    "                op_scores[2] = scores[i-1,j-1] + m\n",
    "                op_scores[3] = min_val\n",
    "            else:\n",
    "                op_scores[2] = min_val\n",
    "                op_scores[3] = scores[i-1,j-1] + mm\n",
    "            # free start gap\n",
    "            op_scores[4] = scores[s1len,j]\n",
    "            \n",
    "            scores[i,j] = op_scores.max()\n",
    "            ops[:5,i,j] = op_scores == scores[i,j]\n",
    "            if ops[4,i,j]:\n",
    "                ops[5,s1len,j] = True # gap end\n",
    "    return scores, ops#np.packbits(ops, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_row(row):\n",
    "    scores,_ = gapped_twinalign(row.query_seq, row.ref1, row.ref2)\n",
    "    #row['score'] = score\n",
    "    return scores[-1,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_align_row(row):\n",
    "    if pd.isnull(row.ref1):\n",
    "        #row['score'] = np.nan\n",
    "        #row['transisions'] = None\n",
    "        return row\n",
    "    qlen, s1len, s2len = len(row.query_seq), len(row.ref1), len(row.ref2)\n",
    "    query, subj = row.query_seq.encode(\"utf8\"), (row.ref1+row.ref2).encode(\"utf8\")\n",
    "    align(query, subj, \n",
    "          qlen, s1len, s2len,\n",
    "          args.match, args.mismatch, args.gap_open, args.gap_extension,\n",
    "          scores_pp, *ops_pp)\n",
    "    score = scores[s1len+s2len, qlen]\n",
    "    #print(row.query_seq, row.ref1, row.ref2)\n",
    "    #return scores[s1len+s2len, qlen]\n",
    "    \n",
    "    transitions = np.zeros(shape=(s1len+1, s2len+1), dtype=np.bool_)\n",
    "    if transitions.flags['C_CONTIGUOUS'] == False:\n",
    "        transitions = np.ascontiguousarray(transitions, dtype=transitions.dtype)\n",
    "    transitions_pp = (transitions.ctypes.data + np.arange(transitions.shape[0]) * transitions.strides[0]).astype(np.uintp)\n",
    "    backtrace(*ops_pp,\n",
    "              qlen, s1len, s2len,\n",
    "              s1len+s2len, qlen, 0, s2len, \n",
    "              transitions_pp)\n",
    "    transitions_hr = list(zip(*np.where(transitions)))\n",
    "    \n",
    "    row['score'] = score\n",
    "    row['transitions'] = transitions_hr\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_align(query_seq, ref1, ref2):\n",
    "    qlen, s1len, s2len = len(query_seq), len(ref1), len(ref2)\n",
    "    query, subj = query_seq.encode(\"utf8\"), (ref1+ref2).encode(\"utf8\")\n",
    "    align(query, subj, \n",
    "          qlen, s1len, s2len,\n",
    "          args.match, args.mismatch, args.gap_open, args.gap_extension,\n",
    "          scores_pp, *ops_pp)\n",
    "    score = scores[s1len+s2len, qlen]\n",
    "    \n",
    "    transitions = np.zeros(shape=(s1len+1, s2len+1), dtype=np.bool_)\n",
    "    if transitions.flags['C_CONTIGUOUS'] == False:\n",
    "        transitions = np.ascontiguousarray(transitions, dtype=transitions.dtype)\n",
    "    transitions_pp = (transitions.ctypes.data + np.arange(transitions.shape[0]) * transitions.strides[0]).astype(np.uintp)\n",
    "    backtrace(*ops_pp,\n",
    "              qlen, s1len, s2len,\n",
    "              s1len+s2len, qlen, 0, s2len, \n",
    "              transitions_pp)\n",
    "    transitions_hr = list(zip(*np.where(transitions)))\n",
    "\n",
    "    print(score)\n",
    "    print(scores[:s1len+s2len+1, :qlen+1])\n",
    "    print(ops[:,:s1len+s2len+1, :qlen+1])\n",
    "    print(transitions_hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg_pat = re.compile(\"(\\d+)([=XID])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_args(['/vol/seqpro2019/MLinder_SMART/Run00376/barcode16.fastq', '/vol/seqpro2019/MLinder_SMART/analysis/Run00376/MIT52.fa', '/vol/seqpro2019/MLinder_SMART/analysis/Run00376/pUC19_ISCg1.fa', '--prefix', 'barcode01', '--wordsize', '12', '--plot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fq_files = []\n",
    "for entry in args.reads:\n",
    "    if os.path.isfile(entry) and (entry.endswith(\".fastq\") or entry.endswith(\".fq\")):\n",
    "        fq_files.append(entry)\n",
    "    else:\n",
    "        fq_files.extend([os.path.join(entry, f) for f in os.listdir(entry) if os.path.isfile(os.path.join(entry, f)) \\\n",
    "            and (f.endswith(\".fastq\") or f.endswith(\".fq\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - reading adapter sequence ...\n",
      "          1 adapter sequence(s) in fasta file\n",
      " - reading genome fasta ...\n",
      "          1 genomic sequence(s) in fasta file\n",
      " - reading fastq files ...\n",
      "     272410 reads in dataset\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adapter = {}\n",
    "print(\" - reading adapter sequence ...\")\n",
    "for record in SeqIO.parse(args.adapter, \"fasta\"):\n",
    "    adapter[str(record.id)] = str(record.seq)\n",
    "adapter = pd.DataFrame.from_dict(adapter, orient='index', columns=['seq'], dtype='string')\n",
    "print('{:>11} adapter sequence(s) in fasta file'.format(len(adapter)))\n",
    "\n",
    "print(\" - reading genome fasta ...\")\n",
    "genome = {}\n",
    "for record in SeqIO.parse(args.genome, \"fasta\"):\n",
    "    genome[str(record.id)] = str(record.seq)\n",
    "genome = pd.DataFrame.from_dict(genome, orient='index', columns=['seq'], dtype='string')\n",
    "print('{:>11} genomic sequence(s) in fasta file'.format(len(genome)))\n",
    "\n",
    "print(\" - reading fastq files ...\")\n",
    "reads = {}\n",
    "#reads = pd.DataFrame([], {'seq':Seq})\n",
    "for fqFile in fq_files:\n",
    "    for record in SeqIO.parse(fqFile, \"fastq\"):\n",
    "        reads[str(record.id)] = str(record.seq)\n",
    "reads = pd.DataFrame.from_dict(reads, orient='index', columns=['seq'], dtype='string')\n",
    "print(\"{:>11} reads in dataset\\n\".format(len(reads)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dir = '{}_tmp'.format(args.prefix)\n",
    "if os.path.exists(tmp_dir):\n",
    "    shutil.rmtree(tmp_dir)\n",
    "os.makedirs(tmp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - performing reads to adapter reference mapping ...\n",
      "     175673         primary alignments against adapter sequence(s)\n",
      "      58740  33.4 % against (+) strand\n",
      "     116933  66.6 % against (-) strand\n",
      "     164011  60.2 % of reads align against any adapter sequence\n"
     ]
    }
   ],
   "source": [
    "print(\" - performing reads to adapter reference mapping ...\")\n",
    "fq_fn = \" \".join(fq_files)\n",
    "ref_fn = args.adapter\n",
    "paf_fn = os.path.join(tmp_dir, \"adapter_alignment.paf\")\n",
    "exit_code = run_minimap2(ref_fn, fq_fn, paf_fn)\n",
    "if exit_code:\n",
    "    print('ERROR: adapter reference mapping failed with exit code', exit_code)\n",
    "    exit(1)\n",
    "ad_algn_df = parse_paf(paf_fn, cigar=True)#.set_index('qid')\n",
    "print(\"{:>11} {:>7} primary alignments against adapter sequence(s)\".format(len(ad_algn_df), \"\"))\n",
    "c = sum(ad_algn_df.strand == '+')\n",
    "print(\"{:>11} {:>5.1f} % against (+) strand\".format(c, c/len(ad_algn_df)*100.))\n",
    "c = sum(ad_algn_df.strand == '-')\n",
    "print(\"{:>11} {:>5.1f} % against (-) strand\".format(c, c/len(ad_algn_df)*100.))\n",
    "c = len(set(ad_algn_df.qid))\n",
    "print(\"{:>11} {:>5.1f} % of reads align against any adapter sequence\".format(c, c/len(reads)*100.))\n",
    "#df = pd.merge(df, algn_df, how='outer', left_index=True, right_index=True, sort=False, suffixes=('_x', '_y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - performing reads to genome reference mapping ...\n",
      "     143593         primary alignments against genomic sequence(s)\n",
      "      95609  66.6 % against (+) strand\n",
      "      47984  33.4 % against (-) strand\n",
      "     133750  49.1 % of reads align against any genome sequence\n"
     ]
    }
   ],
   "source": [
    "print(\" - performing reads to genome reference mapping ...\")\n",
    "ref_fn = args.genome\n",
    "paf_fn = os.path.join(tmp_dir, \"genome_alignment.paf\")\n",
    "exit_code = run_minimap2(ref_fn, fq_fn, paf_fn)\n",
    "if exit_code:\n",
    "    print('ERROR: adapter reference mapping failed with exit code', exit_code)\n",
    "    exit(1)\n",
    "gn_algn_df = parse_paf(paf_fn, cigar=True)#.set_index('qid')\n",
    "print(\"{:>11} {:>7} primary alignments against genomic sequence(s)\".format(len(gn_algn_df), \"\"))\n",
    "c = sum(gn_algn_df.strand == '+')\n",
    "print(\"{:>11} {:>5.1f} % against (+) strand\".format(c, c/len(gn_algn_df)*100.))\n",
    "c = sum(gn_algn_df.strand == '-')\n",
    "print(\"{:>11} {:>5.1f} % against (-) strand\".format(c, c/len(gn_algn_df)*100.))\n",
    "c = len(set(gn_algn_df.qid))\n",
    "print(\"{:>11} {:>5.1f} % of reads align against any genome sequence\".format(c, c/len(reads)*100.))\n",
    "#df = pd.merge(df, algn_df, how='outer', left_index=True, right_index=True, sort=False, suffixes=('_ad', '_gn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - determining statistics about per-base difference ([align. subject len] - [align. query len]) / [align. query len] al from genome alignments\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.0079 mean of per-base difference in sequence length\n",
      "     0.0275 std dev of per-base difference in sequence length\n"
     ]
    }
   ],
   "source": [
    "if not (args.mean and args.std):\n",
    "    print(\" - determining statistics about per-base difference ([align. subject len] - [align. query len]) / [align. query len] al from genome alignments\")\n",
    "    sequence_length_stats(gn_algn_df)\n",
    "    print(\"{:>11.4f} mean of per-base difference in sequence length\".format(args.mean))\n",
    "    print(\"{:>11.4f} std dev of per-base difference in sequence length\".format(args.std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - joining alignment data and filtering for adjacent adapter-genome alignments\n"
     ]
    }
   ],
   "source": [
    "print(\" - joining alignment data and filtering for adjacent adapter-genome alignments\")\n",
    "# determine order of alignments of each read with respect to each adapter-subject pair\n",
    "d = pd.concat([ad_algn_df, gn_algn_df], keys=['ad', 'gn'])\n",
    "for ad_id in adapter.index:\n",
    "    for gn_id in genome.index:\n",
    "        comb = [ad_id, gn_id]\n",
    "        o = d.loc[(d.subj.isin(comb))].sort_values(by=[\"qid\", \"qst\"], ascending=True)\n",
    "        d.loc[o.index, \":\".join(comb)] = np.arange(len(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ad = pd.merge(pd.DataFrame(reads.index, columns=['rid']).set_index('rid', drop=False), d.loc[d.index.droplevel(1) == 'ad'].set_index('qid', drop=False),\n",
    "                how='outer', left_index=True, right_index=True, sort=False)\n",
    "#d_gn = pd.merge(df, d.loc[d.index.droplevel(1) == 'gn'].set_index('qid', drop=False),\n",
    "#                how='outer', left_index=True, right_index=True, sort=False)\n",
    "df = pd.merge(d_ad, \n",
    "              d.loc[d.index.droplevel(1) == 'gn'].set_index('qid', drop=False), \n",
    "              how='outer', left_index=True, right_index=True, sort=False, suffixes=('_ad', '_gn'))\n",
    "# reset index\n",
    "df = df.reset_index(drop=True)\n",
    "sel = df.subj_ad.notnull() & \\\n",
    "      df.subj_gn.notnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     295580         entities after joining\n",
      "     246797  83.5 % not aligning against both an adapter and a genomic seq\n",
      "       7149   2.4 % are entries are alignments that are not adjacent to each other\n",
      "\n",
      "      37564  13.8 % reads remaining that contain potential transitions between adapter and genomic seq.\n",
      "      41634  14.1 % potential transitions remaining\n"
     ]
    }
   ],
   "source": [
    "# determine if the adapter alignment and the genome alignment are adjacent to each other with respect to \n",
    "# all other alignments of the given read to the same adapter and genome sequence\n",
    "for ad_id in adapter.index:\n",
    "    for gn_id in genome.index:\n",
    "        sel_ = (df.subj_ad == ad_id) & (df.subj_gn == gn_id)\n",
    "        df.loc[sel_, 'align_dist'] = (df[sel_][\"{}:{}_ad\".format(ad_id, gn_id)] - df[sel_][\"{}:{}_gn\".format(ad_id, gn_id)]).abs()\n",
    "print('{:>11} {:>7} entities after joining'.format(len(df), \"\"))\n",
    "c = sum(np.logical_not(sel))\n",
    "print('{:>11} {:>5.1f} % not aligning against both an adapter and a genomic seq'.format(c, c/len(df)*100.))\n",
    "c = sum((df.align_dist > 1.))\n",
    "print('{:>11} {:>5.1f} % are entries are alignments that are not adjacent to each other'.format(c, c/len(df)*100.))\n",
    "sel &= (df.align_dist == 1.)\n",
    "print()\n",
    "c = len(set(df.loc[sel, 'rid']))\n",
    "print('{:>11} {:>5.1f} % reads remaining that contain potential transitions between adapter and genomic seq.'.format(c, c/len(reads)*100.))\n",
    "c = sum(sel)\n",
    "print('{:>11} {:>5.1f} % potential transitions remaining'.format(c, c/len(df)*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - deleting all entries from dataframe that are not potential transitions\n"
     ]
    }
   ],
   "source": [
    "# deleting non-potential transitions at this point so that all procentual values are with respect to potential ones only\n",
    "print(\" - deleting all entries from dataframe that are not potential transitions\")\n",
    "df = df.drop(df.index[np.logical_not(sel)]).astype({\"qlen_ad\":np.int32, \"qst_ad\":np.int32, \"qen_ad\":np.int32, \"slen_ad\":np.int32,\n",
    "                                                   \"sst_ad\":np.int32, \"sen_ad\":np.int32, \"mlen_ad\":np.int32, \"blen_ad\":np.int32, \"mapq_ad\":np.int32,\n",
    "                                                   \"qlen_gn\":np.int32, \"qst_gn\":np.int32, \"qen_gn\":np.int32, \"slen_gn\":np.int32,\n",
    "                                                   \"sst_gn\":np.int32, \"sen_gn\":np.int32, \"mlen_gn\":np.int32, \"blen_gn\":np.int32, \"mapq_gn\":np.int32})\n",
    "sel = df.subj_ad.notnull() & \\\n",
    "      df.subj_gn.notnull() & \\\n",
    "      (df.align_dist == 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      24639  59.2 % of total potential transpositions are adapter -> genome\n",
      "      16995  40.8 % of total potential transpositions are genome -> adapter\n",
      " - filter potential subject transitions based on alignment lengths\n",
      "        231   0.6 % of total potential transpositions filtered\n",
      "         50   0.1 % bc adapter alignment length < 50\n",
      "        181   0.4 % bc genome alignment length < 50\n",
      "      41403  99.4 % potential transitions remaining\n"
     ]
    }
   ],
   "source": [
    "# identify if row describes a transition from adapter to genomic sequence or vise versa\n",
    "df.loc[sel,'trans_order'] = 0 # qst_ad == qst_gn\n",
    "df.loc[sel & (df.qst_ad < df.qst_gn), 'trans_order'] = 1 # adapter -> genome\n",
    "df.loc[sel & (df.qst_ad > df.qst_gn), 'trans_order'] = -1 # genome -> adapter\n",
    "c = sum(df[sel].qst_ad < df[sel].qst_gn)\n",
    "print('{:>11} {:>5.1f} % of total potential transpositions are adapter -> genome'.format(c, c/len(df[sel])*100.))\n",
    "c = sum(df[sel].qst_ad > df[sel].qst_gn)\n",
    "print('{:>11} {:>5.1f} % of total potential transpositions are genome -> adapter'.format(c, c/len(df[sel])*100.))\n",
    "\n",
    "# select all entries with an sufficiently long alignment to both the adapter and the genome\n",
    "print(' - filter potential subject transitions based on alignment lengths')\n",
    "sel_ = (df.blen_ad >= args.min_adapter_blen) & \\\n",
    "       (df.blen_gn >= args.min_genome_blen)\n",
    "c = sum(sel & np.logical_not(sel_))\n",
    "print('{:>11} {:>5.1f} % of total potential transpositions filtered'.format(c, c/len(df)*100.))\n",
    "c = sum(sel & np.logical_not(df.blen_ad >= args.min_adapter_blen))\n",
    "print('{:>11} {:>5.1f} % bc adapter alignment length < {}'.format(c, c/len(df)*100., args.min_adapter_blen))\n",
    "c = sum(sel & np.logical_not(df.blen_gn >= args.min_genome_blen))\n",
    "print('{:>11} {:>5.1f} % bc genome alignment length < {}'.format(c, c/len(df)*100., args.min_genome_blen))\n",
    "sel &= sel_\n",
    "c = sum(sel)\n",
    "print('{:>11} {:>5.1f} % potential transitions remaining'.format(c, c/len(df)*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - determine query seq start and end in read coordinates\n",
      "          0   0.0 % of remaining potential transpositions had < 12 matching terminal bases\n",
      "      41403  99.4 % potential transitions remaining\n"
     ]
    }
   ],
   "source": [
    "# determine query seq start and end in read coordinates\n",
    "print(\" - determine query seq start and end in read coordinates\")\n",
    "df = get_query_seq(df, sel)\n",
    "c = sum(sel & (df.query_st.isnull() | df.query_en.isnull()))\n",
    "print('{:>11} {:>5.1f} % of remaining potential transpositions had < {} matching terminal bases'.format(c, c/sum(sel)*100., args.wordsize))\n",
    "sel &= df.query_st.notnull() & df.query_en.notnull()\n",
    "c = sum(sel)\n",
    "print('{:>11} {:>5.1f} % potential transitions remaining'.format(c, c/len(df)*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - exclude entries based on query sequence length (adapter-genome alignment distance) from analysis\n",
      "        504   1.2 % of remaining potential transpositions removed due to query seq. length < 0 nt (alignment overlap of more than 5 nt)\n",
      "        579   1.4 % of remaining potential transpositions removed due to query seq. length > 200 nt\n",
      "      40320  96.8 % potential transitions remaining\n"
     ]
    }
   ],
   "source": [
    "# filter out entries with query seq. that are too long -> distance between adapter and genome alignment is too large\n",
    "print(\" - exclude entries based on query sequence length (adapter-genome alignment distance) from analysis\")\n",
    "too_short = (df.query_en - df.query_st) < 0.\n",
    "too_long = (df.query_en - df.query_st) > args.max_dist\n",
    "c = sum(too_short)\n",
    "print('{:>11} {:>5.1f} % of remaining potential transpositions removed due to query seq. length < 0 nt (alignment overlap of more than {} nt)'.format(c, c/sum(sel)*100., args.strip))\n",
    "c = sum(too_long)\n",
    "print('{:>11} {:>5.1f} % of remaining potential transpositions removed due to query seq. length > {} nt'.format(c, c/sum(sel)*100., args.max_dist))\n",
    "sel &= np.logical_not(too_short) & np.logical_not(too_long)\n",
    "c = sum(sel)\n",
    "print('{:>11} {:>5.1f} % potential transitions remaining'.format(c, c/len(df)*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - determining query sequences\n"
     ]
    }
   ],
   "source": [
    "# set query seq for each row\n",
    "print(\" - determining query sequences\")\n",
    "df.loc[sel, 'query_seq'] = df[sel].apply(lambda row: reads.loc[row.rid].seq[int(row.query_st) : int(row.query_en)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - determining reference sequences\n"
     ]
    }
   ],
   "source": [
    "# determine reference sequences\n",
    "print(\" - determining reference sequences\")\n",
    "#df.loc[sel, 'min_ref_len'] = ((df[sel].query_en - df[sel].query_st) * (1 + args.mean - 3 * args.std) - 0.5).round()\n",
    "df.loc[sel, 'max_ref_len'] = ((df[sel].query_en - df[sel].query_st) * (1 + args.mean + 3 * args.std) + 0.5).round()\n",
    "\n",
    "# set first and second reference seq\n",
    "sel_ = sel & (df.trans_order == 1.) & (df.strand_ad == '+')\n",
    "df.loc[sel_, 'ref1'] = df[sel_].apply(\n",
    "    lambda row: adapter.loc[row.subj_ad].seq[(row.sen_ad - row.ref1_trim) : (row.sen_ad - row.ref1_trim) + int(row.max_ref_len)], axis=1)\n",
    "sel_ = sel & (df.trans_order == 1.) & (df.strand_ad == '-')\n",
    "df.loc[sel_, 'ref1'] = df[sel_].apply(\n",
    "    lambda row: adapter.loc[row.subj_ad].seq[(row.sst_ad + row.ref1_trim) - int(row.max_ref_len) : (row.sst_ad + row.ref1_trim)], axis=1)\n",
    "df.loc[sel_, 'ref1'] = df.loc[sel_, 'ref1'].str.translate(compl).str[::-1] # reverse complement\n",
    "\n",
    "\n",
    "sel_ = sel & (df.trans_order == 1.) & (df.strand_gn == '+')\n",
    "df.loc[sel_, 'ref2'] = df[sel_].apply(\n",
    "    lambda row: genome.loc[row.subj_gn].seq[(row.sst_gn + row.ref2_trim) - int(row.max_ref_len) : (row.sst_gn + row.ref2_trim)], axis=1)\n",
    "sel_ = sel & (df.trans_order == 1.) & (df.strand_gn == '-')\n",
    "df.loc[sel_, 'ref2'] = df[sel_].apply(\n",
    "    lambda row: genome.loc[row.subj_gn].seq[(row.sen_gn - row.ref2_trim) : (row.sen_gn - row.ref2_trim) + int(row.max_ref_len)], axis=1)\n",
    "df.loc[sel_, 'ref2'] = df.loc[sel_, 'ref2'].str.translate(compl).str[::-1] # reverse complement\n",
    "\n",
    "\n",
    "sel_ = sel & (df.trans_order == -1.) & (df.strand_gn == '+')\n",
    "df.loc[sel_, 'ref1'] = df[sel_].apply(\n",
    "    lambda row: genome.loc[row.subj_gn].seq[(row.sen_gn - row.ref1_trim) : (row.sen_gn - row.ref1_trim) + int(row.max_ref_len)], axis=1)\n",
    "sel_ = sel & (df.trans_order == -1.) & (df.strand_gn == '-')\n",
    "df.loc[sel_, 'ref1'] = df[sel_].apply(\n",
    "    lambda row: genome.loc[row.subj_gn].seq[(row.sst_gn + row.ref1_trim) - int(row.max_ref_len) : (row.sst_gn + row.ref1_trim)], axis=1)\n",
    "df.loc[sel_, 'ref1'] = df.loc[sel_, 'ref1'].str.translate(compl).str[::-1] # reverse complement\n",
    "\n",
    "\n",
    "sel_ = sel & (df.trans_order == -1.) & (df.strand_ad == '+')\n",
    "df.loc[sel_, 'ref2'] = df[sel_].apply(\n",
    "    lambda row: adapter.loc[row.subj_ad].seq[(row.sst_ad + row.ref2_trim) - int(row.max_ref_len) : (row.sst_ad + row.ref2_trim)], axis=1)\n",
    "sel_ = sel & (df.trans_order == -1.) & (df.strand_ad == '-')\n",
    "df.loc[sel_, 'ref2'] = df[sel_].apply(\n",
    "    lambda row: adapter.loc[row.subj_ad].seq[(row.sen_ad - row.ref2_trim) : (row.sen_ad - row.ref2_trim) + int(row.max_ref_len)], axis=1)\n",
    "df.loc[sel_, 'ref2'] = df.loc[sel_, 'ref2'].str.translate(compl).str[::-1] # reverse complement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - aligning\n",
      "/vol/coryne/mhaak/crossalign/align.so\n",
      "438\n"
     ]
    }
   ],
   "source": [
    "print(' - aligning')\n",
    "# initialize the necessary data structures\n",
    "wd = !pwd\n",
    "print(os.path.join(str(wd[0]), \"align.so\"))\n",
    "clib = os.path.join(str(wd[0]), \"align.so\")\n",
    "\n",
    "nd_pp = np.ctypeslib.ndpointer(dtype=np.uintp, ndim=1, flags='C_CONTIGUOUS')\n",
    "align = ct.CDLL(clib).align\n",
    "align.argtypes = [ct.c_char_p, ct.c_char_p, \n",
    "                  ct.c_short, ct.c_short, ct.c_short,\n",
    "                  ct.c_short, ct.c_short, ct.c_short, ct.c_short,\n",
    "                  nd_pp, nd_pp, nd_pp, nd_pp, nd_pp, nd_pp, nd_pp]\n",
    "align.restype = ct.c_int\n",
    "\n",
    "backtrace = ct.CDLL(clib).backtrace\n",
    "backtrace.argtypes = [nd_pp, nd_pp, nd_pp, nd_pp, nd_pp, nd_pp,\n",
    "                      ct.c_short, ct.c_short, ct.c_short,\n",
    "                      ct.c_short, ct.c_short, ct.c_short, ct.c_short,\n",
    "                      nd_pp]\n",
    "backtrace.restype = ct.c_int\n",
    "\n",
    "ct.CDLL(clib).init()\n",
    "\n",
    "max_subj_len = 2*int((args.max_dist * (1 + args.mean + 3 * args.std) + 0.5).round())\n",
    "print(max_subj_len)\n",
    "scores = np.empty(shape=(max_subj_len+1, args.max_dist+1), dtype=np.int16)\n",
    "ops = np.empty(shape=(6,max_subj_len+1, args.max_dist+1), dtype=np.bool_)\n",
    "#m, mm, go, ge = 1, -2, -4, -1\n",
    "# initialize the fields that never change\n",
    "scores[0,0] = 0\n",
    "scores[0,1:] = np.arange(args.gap_open + args.gap_extension, args.gap_open + (args.max_dist+1)*args.gap_extension, args.gap_extension)\n",
    "ops[:,0,:] = False\n",
    "ops[:,:,0] = False\n",
    "ops[0,0,1:] = True\n",
    "ops[2,0,0] = True\n",
    "ops[5,0,0] = True\n",
    "if scores.flags['C_CONTIGUOUS'] == False:\n",
    "    scores = np.ascontiguousarray(scores, dtype=scores.dtype)\n",
    "if ops.flags['C_CONTIGUOUS'] == False:\n",
    "    ops = np.ascontiguousarray(ops, dtype=ops.dtype)\n",
    "scores_pp = (scores.ctypes.data + np.arange(scores.shape[0]) * scores.strides[0]).astype(np.uintp)\n",
    "ops_pp = [(ops[i].ctypes.data + np.arange(ops[i].shape[0]) * ops[i].strides[0]).astype(np.uintp) for i in range(6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 1 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "pandarallel.initialize(nb_workers=1, progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "8\n",
      "14\n",
      "15\n",
      "25\n",
      "26\n",
      "28\n",
      "42\n",
      "44\n",
      "48\n",
      "55\n",
      "62\n",
      "68\n",
      "69\n",
      "82\n",
      "86\n",
      "93\n",
      "98\n",
      "102\n",
      "125\n",
      "126\n",
      "132\n",
      "133\n",
      "143\n",
      "145\n",
      "152\n",
      "166\n",
      "167\n",
      "173\n",
      "179\n",
      "185\n",
      "187\n",
      "190\n",
      "206\n",
      "215\n",
      "222\n",
      "227\n",
      "237\n",
      "240\n",
      "244\n",
      "252\n",
      "263\n",
      "264\n",
      "279\n",
      "282\n",
      "299\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "308\n",
      "313\n",
      "315\n",
      "354\n"
     ]
    }
   ],
   "source": [
    "for i,row in df[:500].iterrows():\n",
    "    print(i)\n",
    "    c_align_row(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAGCCAGTTAACTGGATCAGCTGTCCCCGTCTGAAACCATTCCCACTCAAGATTAACAATATGTACGCCCTTCTTCGCTTCGGCGGGTAGCTCATCCAACCATTGCGCCGTCTTTTTGTCGACTACCATGCCCACCAACGAGATCGGAAGAGCACACGTCTGCGTGC GAGCCGCAAAATACTGCTCAGACGCGTTAGAGTGCATTGATCTTATGGACCAACTGCCCTGAATGGATAAGGCACCGCAGAATGTAGTGGTTCAAATTACGGAAACCTAGAGCAATCCCACGCAAATGCTCCAACCGTCCGTTGATCGCTTCGACCGGACCGTTGGAGACACCAACATCGAAA GCGGCCAGTCGATCATGTTGAGGTTGACGTTGGCGTAATGCGCGTCGGGGGACCAATGATCGTTGACGAGGTAGGTGTACAGCGTGCCGTCCTCGAGCGCCTGGAAGCCGTTTGCCACACCACGCGGGACGTAAACTCCCACGTCAGGGGTAATTTTTTGCGTTACGACGTTACCGTACGTGC\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[354].query_seq, df.loc[354].ref1, df.loc[354].ref2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "c_align_row(df.loc[354])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# start the alignment\n",
    "df[20:500].apply(lambda row: c_align_row(row), axis=1)\n",
    "#df.loc[sel & (df.query_seq.str.len() > 0), 'norm_score'] = df[sel].score / df[sel].query_seq.str.len()\n",
    "#df.loc[sel & (df.query_seq.str.len() == 0), 'norm_score'] = args.match # if the two alignments are fitting together perfetly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[sel & (df.query_seq.str.len() == 0), [\"query_seq\", \"ref1\", \"ref2\", \"score\", \"norm_score\", \"transitions\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_norm_score_distribution(df, nbins=50):\n",
    "    x, y = df.query_seq.str.len(), df.norm_score\n",
    "    \n",
    "    # definitions for the axes\n",
    "    left, width = 0.1, 0.65\n",
    "    bottom, height = 0.1, 0.65\n",
    "    spacing = 0.005\n",
    "\n",
    "    rect_scatter = [left, bottom, width, height]\n",
    "    rect_histx = [left, bottom + height + spacing, width, 0.2]\n",
    "    rect_histy = [left + width + spacing, bottom, 0.2, height]\n",
    "\n",
    "    # start with a rectangular Figure\n",
    "    plt.figure(figsize=(8, 8))\n",
    "\n",
    "    ax_scatter = plt.axes(rect_scatter)\n",
    "    ax_scatter.tick_params(direction='in', top=True, right=True)\n",
    "    ax_histx = plt.axes(rect_histx)\n",
    "    ax_histx.tick_params(direction='in', labelbottom=False)\n",
    "    ax_histy = plt.axes(rect_histy)\n",
    "    ax_histy.tick_params(direction='in', labelleft=False)\n",
    "\n",
    "    ax_scatter.scatter(x, y, s=1., alpha=0.05)\n",
    "\n",
    "    ax_histx.hist(x, bins=nbins)\n",
    "    ax_histy.hist(y, bins=nbins, orientation='horizontal')\n",
    "\n",
    "    ax_histx.set_xlim(ax_scatter.get_xlim())\n",
    "    ax_histy.set_ylim(ax_scatter.get_ylim())\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.plot:\n",
    "    plot_norm_score_distribution(df[sel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d = df.loc[sel & (df.norm_score >= 0.5), ['trans_order','strand_ad','subj_ad','sst_ad', 'sen_ad', 'subj_gn','sst_gn', 'sen_gn','strand_gn','max_ref_len','norm_score','transitions']].explode('transitions')\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set first and second reference seq\n",
    "sel_ = (d.trans_order == 1.) & (d.strand_ad == '+')\n",
    "d.loc[sel_, 'ts'] = d[sel_].apply(\n",
    "    lambda row: (row.sen_ad - args.strip) + row.transitions[0], axis=1)\n",
    "    #lambda row: adapter.loc[row.subj_ad].seq[(row.sen_ad - args.strip) : (row.sen_ad - args.strip) + int(row.max_ref_len)], axis=1)\n",
    "sel_ = (d.trans_order == 1.) & (d.strand_ad == '-')\n",
    "d.loc[sel_, 'ts'] = d[sel_].apply(\n",
    "    lambda row: (row.sst_ad + args.strip) - row.transitions[0], axis=1)\n",
    "    #lambda row: adapter.loc[row.subj_ad].seq[(row.sst_ad + args.strip) - int(row.max_ref_len) : (row.sst_ad + args.strip)], axis=1)\n",
    "\n",
    "sel_ = (d.trans_order == 1.) & (d.strand_gn == '+')\n",
    "d.loc[sel_, 'te'] = d[sel_].apply(\n",
    "    lambda row: (row.sst_gn + args.strip) - int(row.max_ref_len) + row.transitions[1], axis=1)\n",
    "    #lambda row: genome.loc[row.subj_gn].seq[(row.sst_gn + args.strip) - int(row.max_ref_len) : (row.sst_gn + args.strip)], axis=1)\n",
    "sel_ = (d.trans_order == 1.) & (d.strand_gn == '-')\n",
    "d.loc[sel_, 'te'] = d[sel_].apply(\n",
    "    lambda row: (row.sen_gn - args.strip) + int(row.max_ref_len) - row.transitions[1], axis=1)\n",
    "    #lambda row: genome.loc[row.subj_gn].seq[(row.sen_gn - args.strip) : (row.sen_gn - args.strip) + int(row.max_ref_len)], axis=1)\n",
    "\n",
    "sel_ = (d.trans_order == -1.) & (d.strand_gn == '+')\n",
    "d.loc[sel_, 'ts'] = d[sel_].apply(\n",
    "    lambda row: (row.sen_gn - args.strip) + row.transitions[0], axis=1)\n",
    "    #lambda row: genome.loc[row.subj_gn].seq[(row.sen_gn - args.strip) : (row.sen_gn - args.strip) + int(row.max_ref_len)], axis=1)\n",
    "sel_ = (d.trans_order == -1.) & (d.strand_gn == '-')\n",
    "d.loc[sel_, 'ts'] = d[sel_].apply(\n",
    "    lambda row: (row.sst_gn + args.strip) - row.transitions[0], axis=1)\n",
    "    #lambda row: genome.loc[row.subj_gn].seq[(row.sst_gn + args.strip) - int(row.max_ref_len) : (row.sst_gn + args.strip)], axis=1)\n",
    "\n",
    "sel_ = (d.trans_order == -1.) & (d.strand_ad == '+')\n",
    "d.loc[sel_, 'te'] = d[sel_].apply(\n",
    "    lambda row: (row.sst_ad + args.strip) - int(row.max_ref_len) + row.transitions[1], axis=1)\n",
    "    #lambda row: adapter.loc[row.subj_ad].seq[(row.sst_ad + args.strip) - int(row.max_ref_len) : (row.sst_ad + args.strip)], axis=1)\n",
    "sel_ = (df.trans_order == -1.) & (df.strand_ad == '-')\n",
    "d.loc[sel_, 'te'] = d[sel_].apply(\n",
    "    lambda row: (row.sen_ad - args.strip) + int(row.max_ref_len) - row.transitions[1], axis=1)\n",
    "    #lambda row: adapter.loc[row.subj_ad].seq[(row.sen_ad - args.strip) : (row.sen_ad - args.strip) + int(row.max_ref_len)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
